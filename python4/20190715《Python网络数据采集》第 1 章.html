<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修20190715《Python网络数据采集》第 1 章' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>20190715《Python网络数据采集》第 1 章</center></div><div class='banquan'>原文出处:本文由博客园博主ElonJiang提供。<br/>
原文连接:https://www.cnblogs.com/ElonJiang/p/11194206.html</div><br>
    <p>《Python网络数据采集》7月8号-7月10号，这三天将该书精读一遍，脑海中有了一个爬虫大体框架后，对于后续学习将更加有全局感。</p>
<p>此前，曾试验看视频学习，但是一个视频基本2小时，全部拿下需要30多个视频，如此看来每天学习一个视频时间都是非常吃力的，且都属于被动输入，尤其是在早上学习视频容易犯困。</p>
<p>故此，及时调整学习策略，采用&nbsp;<strong>&ldquo;电子书+廖雪峰网页教程+实操+Google+咨询程序员+每日总结归纳&rdquo;&nbsp;</strong>的主动学习模式，如此更加高效，更加容易把控进度！</p>
<p>学习爬虫，一者兴趣，致力于借此兴趣驱动力掌握编程思维，进而让自己有能够将想法做成产品的的技能；二者，为了一个近在眼前的爬虫商业化机遇，更希望借此为自己增加一个收入来源。</p>
<p>&nbsp;</p>
<p>1. 爬虫常见得异常及处理方法，用一个简单得爬虫代码解释，核心知识点：</p>
<p>（1）异常一：网页在服务器上不存在（或者获取页面时，出现错误）。该异常发生时，程序会返回HTTP错误，如&ldquo;404 Page Not Found&rdquo; "500 Internet Server Error"等。</p>
<p>（2）异常二：服务器不存在（即，链接打不开，或者URL链接写错了），这时，urlopen会返回一个None对象。</p>
<p>Ps：有的时候，网页已经从服务器成功获取，如果网页上的内容并非完全是我们期望的那样，也会出现异常。</p>
<pre><code></pre>
<div class="cnblogs_code">
<pre><code> 1 from urllib.request import<span> urlopen
 2 from bs4 import<span> BeautifulSoup
 3 
 4 try<span>:
 5     html = urlopen("http://pythonscraping.com/pages/page1.html"<span>)
 6 # print(html.read())
 7 # 检测：网页在服务器上是否存在（或者获取页面时是否出现错误）
 8 except<span> HTTPError as e:
 9     print<span>(e)
10 else<span>:
11     bsobj =<span> BeautifulSoup(html.read())
12     # 检测：服务器是否存在（就是说链接能否打开，或者是URL链接写错了）
13     if html is<span> None:
14         print("URL is not found"<span>)
15     else<span>:
16         print<span>(bsobj.h1)
17         # print(bsobj.title)</span></span></span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<div class="cnblogs_code">
<pre><code> 1 # 以上代码更改为检测异常更全面、可读性更强的代码，如下：
 2 from urllib.request import<span> urlopen
 3 from bs4 import<span> BeautifulSoup
 4 
 5 def<span> getTitle(url):
 6     try<span>:
 7         html =<span> urlopen(url)
 8     except<span> HTTPError as e:
 9         return<span> None
10     try<span>:
11         bsobj =<span> BeautifulSoup(html.read())
12         title =<span> bsobj.body.h1
13     except<span> AttributeError as e:
14         return
15     return<span> title
16 
17 title1 = getTitle("http://pythonscraping.com/pages/page1.html"<span>)
18 if title1 ==<span> None:
19     print("Title could not be found"<span>)
20 else<span>:
21     print(title1)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;该部分代码执行时，出现报错：</p>
<div class="cnblogs_code">&nbsp;indentationerror: unexpected indent process finished with exit code 1</div>
<p>&nbsp;Google发现，Tag和Space不能混合使用。原始第五行，def被tab缩进，后删除该tab缩进，问题解决。该问题具体原因，仍需要仔细查明！！！</p>
<pre><code><br /><br /></pre>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>