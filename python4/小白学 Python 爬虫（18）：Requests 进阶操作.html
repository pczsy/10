<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修小白学 Python 爬虫（18）：Requests 进阶操作' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>小白学 Python 爬虫（18）：Requests 进阶操作</center></div><div class='banquan'>原文出处:本文由博客园博主极客挖掘机提供。<br/>
原文连接:https://www.cnblogs.com/babycomeon/p/12042023.html</div><br>
    <p><img src="./images/小白学 Python 爬虫（18）：Requests 进阶操作0.png" /></p>
<blockquote>
<p>人生苦短，我用 Python</p>
</blockquote>
<p>前文传送门：</p>
<p><a href="https://www.geekdigging.com/2019/11/13/3303836941/">小白学 Python 爬虫（1）：开篇</a></p>
<p><a href="https://www.geekdigging.com/2019/11/20/2586166930/">小白学 Python 爬虫（2）：前置准备（一）基本类库的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/21/1005563697/">小白学 Python 爬虫（3）：前置准备（二）Linux基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/22/3679472340/">小白学 Python 爬虫（4）：前置准备（三）Docker基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/24/334078215/">小白学 Python 爬虫（5）：前置准备（四）数据库基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/25/1881661601/">小白学 Python 爬虫（6）：前置准备（五）爬虫框架的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/26/1197821400/">小白学 Python 爬虫（7）：HTTP 基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/27/101847406/">小白学 Python 爬虫（8）：网页基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/28/1668465912/">小白学 Python 爬虫（9）：爬虫基础</a></p>
<p><a href="https://www.geekdigging.com/2019/12/01/2475257648/">小白学 Python 爬虫（10）：Session 和 Cookies</a></p>
<p><a href="https://www.geekdigging.com/2019/12/02/2333822325/">小白学 Python 爬虫（11）：urllib 基础使用（一）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/03/819896244/">小白学 Python 爬虫（12）：urllib 基础使用（二）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/04/2992515886/">小白学 Python 爬虫（13）：urllib 基础使用（三）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/05/104488944/">小白学 Python 爬虫（14）：urllib 基础使用（四）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/07/2788855167/">小白学 Python 爬虫（15）：urllib 基础使用（五）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/09/1691033431/">小白学 Python 爬虫（16）：urllib 实战之爬取妹子图</a></p>
<p><a href="https://www.geekdigging.com/2019/12/10/1910005577/">小白学 Python 爬虫（17）：Requests 基础使用</a></p>
<h2 id="超时设置">超时设置</h2>
<p>前面我们在介绍 urllib 的时候也介绍过超时，我们现在来看下在 Requests 中的超时应该怎么写。</p>
<pre><code><code>import requests

r = requests.get(&quot;https://www.geekdigging.com/&quot;, timeout = 1)
print(r.status_code)</code></pre>
<p>具体的执行结果小编就不贴了。</p>
<p><strong>注意</strong></p>
<p>timeout 仅对连接过程有效，与响应体的下载无关。 timeout 并不是整个下载响应的时间限制，而是如果服务器在 timeout 秒内没有应答，将会引发一个异常（更精确地说，是在 timeout 秒内没有从基础套接字上接收到任何字节的数据时）If no timeout is specified explicitly, requests do not time out.</p>
<h2 id="代理设置">代理设置</h2>
<p>和 urllib 一样，多的介绍就不说了，直接上代码：</p>
<pre><code><code>import requests

proxies = {
  &quot;http&quot;: &quot;http://10.10.1.10:3128&quot;,
  &quot;https&quot;: &quot;http://10.10.1.10:1080&quot;,
}

requests.get(&quot;https://www.geekdigging.com/&quot;, proxies=proxies)</code></pre>
<p>当然，直接运行这个示例可能不行，因为这个代理可能是无效的，可以自己找一些免费的代理进行测试。</p>
<p>而 Requests 除了支持 HTTP 代理，还支持 Socket 代理，因为这是一个可选的功能，在 Requests 的标准库中并未包含，所以在使用前需要先安装。</p>
<pre><code><code>pip install requests[socks]</code></pre>
<p>安装好依赖以后，使用 SOCKS 代理和使用 HTTP 代理一样简单：</p>
<pre><code><code>import requests

proxies_socket = {
    &#39;http&#39;: &#39;socks5://user:pass@host:port&#39;,
    &#39;https&#39;: &#39;socks5://user:pass@host:port&#39;
}

requests.get(&quot;https://www.geekdigging.com/&quot;, proxies = proxies_socket)</code></pre>
<h2 id="cookies">Cookies</h2>
<p>前面我们使用 urllib 处理 Cookies 的时候，写法比较复杂，而使用 Requests 会使证件事情变得非常简单，获取和设置 Cookies 只需一步就能完成。先看一个简单的示例：</p>
<pre><code><code>import requests

r = requests.get(&quot;https://www.csdn.net&quot;)
print(type(r.cookies), r.cookies)
for key, value in r.cookies.items():
    print(key + &#39;=&#39; + value)</code></pre>
<p>结果如下：</p>
<pre><code><code>&lt;class &#39;requests.cookies.RequestsCookieJar&#39;&gt; &lt;RequestsCookieJar[&lt;Cookie dc_session_id=10_1575798031732.659641 for .csdn.net/&gt;, &lt;Cookie uuid_tt_dd=10_19615575150-1575798031732-646184 for .csdn.net/&gt;, &lt;Cookie acw_tc=2760827715757980317314369e26895215355a996a74e112d9936f512dacd1 for www.csdn.net/&gt;]&gt;
dc_session_id=10_1575798031732.659641
uuid_tt_dd=10_19615575150-1575798031732-646184
acw_tc=2760827715757980317314369e26895215355a996a74e112d9936f512dacd1</code></pre>
<p>在 Requests 中我们使用 <code>cookies</code> 属性就可以直接得到 Cookies 。通过打印我们可以发现它的类型是 <code>requests.cookies.RequestsCookieJar</code> ，然后用 <code>items()</code> 方法将其转化为元组组成的列表，遍历输出每一个 Cookie 的名称和值，实现 Cookie 的遍历解析。</p>
<h3 id="通过-cookies-维持会话状态">通过 Cookies 维持会话状态</h3>
<p>因为知乎是需要登录才能访问的，我们选址知乎作为测试的站点，首先直接访问知乎，看下返回的状态码。</p>
<pre><code><code>import requests

r = requests.get(&#39;https://www.zhihu.com&#39;)
print(r.status_code)</code></pre>
<p>结果如下：</p>
<pre><code><code>400</code></pre>
<p>状态码 400 的含义是请求无效（Bad request）。</p>
<p>我们打开浏览器，登录知乎，打开 F12 开发者模式，看下我们登录后的 Cookies 是什么。</p>
<p><img src="./images/小白学 Python 爬虫（18）：Requests 进阶操作1.png" /></p>
<p>我们将这段内容 copy 下来，加到访问头中：</p>
<pre><code><code>import requests

headers = {
    &#39;cookie&#39;: &#39;_zap=7c875737-af7a-4d55-b265-4e3726f8bd30; _xsrf=MU9NN2kHxdMZBVlENJkgnAarY6lFlPmu; d_c0=&quot;ALCiqBcc8Q-PTryJU9ro0XH9RqT4NIEHsMU=|1566658638&quot;; UM_distinctid=16d16b54075bed-05edc85e15710b-5373e62-1fa400-16d16b54076e3d; tst=r; q_c1=1a9d0d0f293f4880806c995d7453718f|1573961075000|1566816770000; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574492254,1574954599,1575721552,1575721901; tgw_l7_route=f2979fdd289e2265b2f12e4f4a478330; CNZZDATA1272960301=1829573289-1568039631-%7C1575793922; capsion_ticket=&quot;2|1:0|10:1575798464|14:capsion_ticket|44:M2FlYTAzMDdkYjIzNDQzZWJhMDcyZGQyZTZiYzA1NmU=|46043c1e4e6d9c381eb18f5dd8e5ca0ddbf6da90cddf10a6845d5d8c589e7754&quot;; z_c0=&quot;2|1:0|10:1575798467|4:z_c0|92:Mi4xLXNyV0FnQUFBQUFBc0tLb0Z4enhEeVlBQUFCZ0FsVk53eFRhWGdBSlc3WFo1Vk5RUThBMHMtanZIQ2tYcGFXV2pn|02268679f394bd32662a43630236c2fd97e439151b0132995db7322736857ab6&quot;; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1575798469&#39;,
    &#39;host&#39;: &#39;www.zhihu.com&#39;,
    &#39;user-agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&#39;
}

r = requests.get(&#39;https://www.zhihu.com&#39;, headers = headers)
print(r.text)</code></pre>
<p>执行结果如下：</p>
<p><img src="./images/小白学 Python 爬虫（18）：Requests 进阶操作2.png" /></p>
<p>结果太长了，小编就直接截个图。</p>
<p>有一点需要注意的是，这里的请求头增加了 UA 和 host ，不然也是无法访问的。</p>
<p>当然除了直接贴这么一串字符串，也是可以通过构造 cookies 的参数来设置 cookies ，这样需要构建一个 RequestsCookieJar 对象，步骤相对会复杂一点，结果是一样的。</p>
<pre><code><code># 构建 RequestsCookieJar 对象
cookies = &#39;_zap=7c875737-af7a-4d55-b265-4e3726f8bd30; _xsrf=MU9NN2kHxdMZBVlENJkgnAarY6lFlPmu; d_c0=&quot;ALCiqBcc8Q-PTryJU9ro0XH9RqT4NIEHsMU=|1566658638&quot;; UM_distinctid=16d16b54075bed-05edc85e15710b-5373e62-1fa400-16d16b54076e3d; tst=r; q_c1=1a9d0d0f293f4880806c995d7453718f|1573961075000|1566816770000; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574492254,1574954599,1575721552,1575721901; tgw_l7_route=f2979fdd289e2265b2f12e4f4a478330; CNZZDATA1272960301=1829573289-1568039631-%7C1575793922; capsion_ticket=&quot;2|1:0|10:1575798464|14:capsion_ticket|44:M2FlYTAzMDdkYjIzNDQzZWJhMDcyZGQyZTZiYzA1NmU=|46043c1e4e6d9c381eb18f5dd8e5ca0ddbf6da90cddf10a6845d5d8c589e7754&quot;; z_c0=&quot;2|1:0|10:1575798467|4:z_c0|92:Mi4xLXNyV0FnQUFBQUFBc0tLb0Z4enhEeVlBQUFCZ0FsVk53eFRhWGdBSlc3WFo1Vk5RUThBMHMtanZIQ2tYcGFXV2pn|02268679f394bd32662a43630236c2fd97e439151b0132995db7322736857ab6&quot;; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1575798469&#39;

jar = requests.cookies.RequestsCookieJar()

for cookie in cookies.split(&#39;;&#39;):
    key, value = cookie.split(&#39;=&#39;, 1)
    jar.set(key, value)

headers_request = {
    &#39;host&#39;: &#39;www.zhihu.com&#39;,
    &#39;user-agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&#39;
}

r = requests.get(&#39;https://www.zhihu.com&#39;, cookies = jar, headers = headers)
print(r.text)</code></pre>
<p>结果是可以访问成功的，小编这里就不贴了。</p>
<p>简单讲一下实现思路，将上面复制下来的 cookies 使用 split() 进行切割，然后使用 set() 方法将 key 和 value 赋值到 RequestsCookieJar 这个对象中，然后在访问知乎的时候将 RequestsCookieJar 赋值给 cookies 参数即可。需要注意的是这里的 header 参数不能少，只是原来 header 里面的 cookies 无需再设置了。</p>
<h2 id="会话维持">会话维持</h2>
<p>接下来这一个是大招，这个功能在 urllib 中是没有的。</p>
<p>先想像一个场景，我们在爬取一个网站的数据，其中有一部分需要使用 GET 请求，有一部分需要使用 POST 请求。但是我们在程序中写一个 get() 再写一个 post() ，实际上是两个不同的会话。</p>
<p>可能有的同学要说了，老师，我们前面讲的会话维持是通过 Cookies 来做的，可以在每次请求的时候添加上 Cookies 。</p>
<p>没问题，这样确实是可以的，但是这么干有些麻烦。 Requests 为我们提供了更加简洁高效的方法—— Session 。</p>
<p>我们还是通过前面介绍过的 https://httpbin.org/ 来进行演示，我们可以通过访问链接：https://httpbin.org/cookies/set/number/123456789 来设置一个 Cookies ，名称叫做 number ，内容是 123456789 。</p>
<p>首先看直接使用 Requests 的示例：</p>
<pre><code><code>import requests

requests.get(&#39;https://httpbin.org/cookies/set/number/123456789&#39;)
r = requests.get(&#39;https://httpbin.org/cookies&#39;)
print(r.text)</code></pre>
<p>结果如下：</p>
<pre><code><code>{
  &quot;cookies&quot;: {}
}</code></pre>
<p>我们直接调用两次 get() 方法，并没有在第二次调用的时候获得 Cookies 。接下来我们换 Session 再看一下：</p>
<pre><code><code>import requests

s = requests.Session()
s.get(&#39;https://httpbin.org/cookies/set/number/123456789&#39;)
r = s.get(&#39;https://httpbin.org/cookies&#39;)
print(r.text)</code></pre>
<p>结果如下：</p>
<pre><code><code>{
  &quot;cookies&quot;: {
    &quot;number&quot;: &quot;123456789&quot;
  }
}</code></pre>
<p>显而易见，我们成功获取到了之前设置的 Cookies 。</p>
<p>所以，利用 Session 可以做到模拟同一个会话而不用手动再去设置 Cookies，它在我们平时的使用中使用的极其广泛，因为它可以模拟在同一个浏览器中访问同一个站点的不同的页面，这在我们爬取很多需要登录的网页时，极大的方便了我们代码的书写。</p>
<h2 id="示例代码">示例代码</h2>
<p>本系列的所有代码小编都会放在代码管理仓库 Github 和 Gitee 上，方便大家取用。</p>
<p><a href="https://github.com/meteor1993/python-learning/tree/master/python-spider/requests-demo" title="示例代码-Github">示例代码-Github</a></p>
<p><a href="https://gitee.com/inwsy/python-learning/tree/master/python-spider/requests-demo" title="示例代码-Gitee">示例代码-Gitee</a></p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>