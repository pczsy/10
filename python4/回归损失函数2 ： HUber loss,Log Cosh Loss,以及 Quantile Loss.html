<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修回归损失函数2 ： HUber loss,Log Cosh Loss,以及 Quantile Loss' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>回归损失函数2 ： HUber loss,Log Cosh Loss,以及 Quantile Loss</center></div><div class='banquan'>原文出处:本文由博客园博主Brook_icv提供。<br/>
原文连接:https://www.cnblogs.com/wangguchangqing/p/12054772.html</div><br>
    <p>均方误差（Mean Square Error,MSE）和平均绝对误差（Mean Absolute Error,MAE) 是回归中最常用的两个损失函数，但是其各有优缺点。为了避免MAE和MSE各自的优缺点，在Faster R-CNN和SSD中使用<span class="math inline">\(\text{Smooth} L_1\)</span>损失函数，当误差在<span class="math inline">\([-1,1]\)</span> 之间时，<span class="math inline">\(\text{Smooth} L_1\)</span>损失函数近似于MSE，能够快速的收敛；在其他的区间则近似于MAE，其导数为<span class="math inline">\(\pm1\)</span>，不会对离群值敏感。</p>
<p>本文再介绍几种回归常用的损失函数</p>
<ul>
<li>Huber Loss</li>
<li>Log-Cosh Loss</li>
<li>Quantile Loss</li>
</ul>
<h3 id="huber-loss">Huber Loss</h3>
<p>Huber损失函数（<span class="math inline">\(\text{Smooth} L_1\)</span>损失函数是其的一个特例）整合了MAE和MSE各自的优点，并避免其缺点<br />
<span class="math display">\[
L_\delta(y,f(x)) = \left \{ \begin{array}{c} \frac{1}{2} (y - f(x))^2 &amp; \mid y - f(x) \mid \leq \delta \\ \delta \mid y-f(x) \mid - \frac{1}{2} \delta ^2 &amp; \text{otherwise}\end{array}\right.
\]</span></p>
<p><span class="math inline">\(\delta\)</span> 是Huber的一个超参数，当真实值和预测值的差值<span class="math inline">\(\mid y- f(x) \mid \leq \delta\)</span> 时，Huber就是MSE；当差值在<span class="math inline">\((-\infty,\delta )\)</span>或者 <span class="math inline">\((\delta,+\infty)\)</span> 时，Huber就是MAE。这样，当误差较大时，使用MAE对离群点不那么敏感；在误差较小时使用MSE，能够快速的收敛；</p>
<p>这里超参数<span class="math inline">\(\delta\)</span>的值的设定就较为重要，和真实值的差值超过该值的样本为异常值。误差的绝对值小于<span class="math inline">\(\delta\)</span> 时，使用MSE；当误差大于<span class="math inline">\(\delta\)</span> 时，使用MAE。</p>
<p>下图给出了不同的<span class="math inline">\(\delta\)</span> 值，Huber的函数曲线。</p>
<p><img src="./images/回归损失函数2 ： HUber loss,Log Cosh Loss,以及 Quantile Loss0.png" /></p>
<p>横轴表示真实值和预测值的差值，纵轴为Huber的函数值。可以看出，<span class="math inline">\(\delta\)</span> 越小其曲线越趋近于MSE；越大，越趋近于MAE。</p>
<p>另外，使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。</p>
<p>在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点。但是，Huber损失的问题是我们可能需要不断调整超参数<span class="math inline">\(\delta\)</span> 。</p>
<blockquote>
<p><span class="math inline">\(\text{Smooth }L_1\)</span> 损失函数可以看作超参数<span class="math inline">\(\delta = 1\)</span> 的Huber函数。</p>
</blockquote>
<h3 id="log-cosh-loss">Log-Cosh Loss</h3>
<p>Log-Cosh是比<span class="math inline">\(L_2\)</span> 更光滑的损失函数，是误差值的双曲余弦的对数<br />
<span class="math display">\[
L(y,f(x)) = \sum_{i=1}^n\log \cosh(y-f(x))
\]</span><br />
其中，<span class="math inline">\(y\)</span>为真实值，<span class="math inline">\(f(x)\)</span> 为预测值。</p>
<p>对于较小的误差$\mid y - f(x) \mid $ ，其近似于MSE，收敛下降较快；对于较大的误差<span class="math inline">\(\mid y - f(x) \mid\)</span> 其近似等于<span class="math inline">\(\mid y-f(x) \mid - log(2)\)</span> ,类似于MAE，不会受到离群点的影响。 Log-Cosh具有Huber 损失的所有有点，且不需要设定超参数。</p>
<p>相比于Huber,Log-Cosh求导比较复杂，计算量较大，在深度学习中使用不多。不过，Log-Cosh处处二阶可微，这在一些机器学习模型中，还是很有用的。例如XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如XGBoost这类机器学习框架，损失函数的二阶可微是很有必要的。但Log-cosh损失也并非完美，其仍存在某些问题。比如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。</p>
<h3 id="quantile-loss-分位数损失">Quantile Loss 分位数损失</h3>
<p>通常的回归算法是拟合训练数据的期望或者中位数，而使用分位数损失函数可以通过给定不同的分位点，拟合训练数据的不同分位数。 如下图</p>
<p><img src="./images/回归损失函数2 ： HUber loss,Log Cosh Loss,以及 Quantile Loss1.png" /></p>
<p>设置不同的分位数可以拟合出不同的直线。</p>
<p>分位数损失函数如下：<br />
<span class="math display">\[
L_{quantile} = \frac{1}{N}\sum_{i=1}^N \amalg_{y &gt; f(x)}(1-\gamma)\mid y-f(x)\mid + \amalg_{y &lt; f(x)}\gamma \mid y - f(x) \mid
\]</span><br />
该函数是一个分段函数，<span class="math inline">\(\gamma\)</span> 为分位数系数，<span class="math inline">\(y\)</span>为真实值，<span class="math inline">\(f(x)\)</span>为预测值。根据预测值和真实值的大小，分两种情况来开考虑。<span class="math inline">\(y &gt; f(x)\)</span> 为高估，预测值比真实值大；<span class="math inline">\(y &lt; f(x)\)</span>为低估，预测值比真实值小，<strong>使用不同过得系数来控制高估和低估在整个损失值的权重</strong> 。</p>
<p>特别的，当<span class="math inline">\(\gamma = 0.5\)</span> 时，分位数损失退化为平均绝对误差MAE，也可以将MAE看成是分位数损失的一个特例 - 中位数损失。下图是取不同的中位点<span class="math inline">\([0.25,0.5,0.7]\)</span> 得到不同的分位数损失函数的曲线，也可以看出0.5时就是MAE。</p>
<p><img src="./images/回归损失函数2 ： HUber loss,Log Cosh Loss,以及 Quantile Loss2.png" /></p>
<h3 id="总结">总结</h3>
<p>均方误差（Mean Square Error,MSE）和平均绝对误差（Mean Absolute Error,MAE) 可以说是回归损失函数的基础。但是MSE对对离群点（异常值）较敏感，MAE在梯度下降的过程中收敛较慢，就出现各种样的分段损失函数，在loss值较小的区间使用MSE，loss值较大的区间使用MAE。</p>
<ul>
<li>Huber Loss ，需要一个超参数<span class="math inline">\(\delta\)</span> ,来定义离群值。$ \text{smooth } L_1$ 是<span class="math inline">\(\delta = 1\)</span> 的一种情况。</li>
<li>Log-Cosh Loss, Log-Cosh是比<span class="math inline">\(L_2\)</span> 更光滑的损失函数，是误差值的双曲余弦的对数.</li>
<li>Quantile Loss , 分位数损失，则可以设置不同的分位点，控制高估和低估在loss中占的比重。</li>
</ul>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>