<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修小白学 Python 爬虫（25）：爬取股票信息' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>小白学 Python 爬虫（25）：爬取股票信息</center></div><div class='banquan'>原文出处:本文由博客园博主极客挖掘机提供。<br/>
原文连接:https://www.cnblogs.com/babycomeon/p/12089531.html</div><br>
    <p><img src="./images/小白学 Python 爬虫（25）：爬取股票信息0.png" /></p>
<blockquote>
<p>人生苦短，我用 Python</p>
</blockquote>
<p>前文传送门：</p>
<p><a href="https://www.geekdigging.com/2019/11/13/3303836941/">小白学 Python 爬虫（1）：开篇</a></p>
<p><a href="https://www.geekdigging.com/2019/11/20/2586166930/">小白学 Python 爬虫（2）：前置准备（一）基本类库的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/21/1005563697/">小白学 Python 爬虫（3）：前置准备（二）Linux基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/22/3679472340/">小白学 Python 爬虫（4）：前置准备（三）Docker基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/24/334078215/">小白学 Python 爬虫（5）：前置准备（四）数据库基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/25/1881661601/">小白学 Python 爬虫（6）：前置准备（五）爬虫框架的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/26/1197821400/">小白学 Python 爬虫（7）：HTTP 基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/27/101847406/">小白学 Python 爬虫（8）：网页基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/28/1668465912/">小白学 Python 爬虫（9）：爬虫基础</a></p>
<p><a href="https://www.geekdigging.com/2019/12/01/2475257648/">小白学 Python 爬虫（10）：Session 和 Cookies</a></p>
<p><a href="https://www.geekdigging.com/2019/12/02/2333822325/">小白学 Python 爬虫（11）：urllib 基础使用（一）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/03/819896244/">小白学 Python 爬虫（12）：urllib 基础使用（二）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/04/2992515886/">小白学 Python 爬虫（13）：urllib 基础使用（三）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/05/104488944/">小白学 Python 爬虫（14）：urllib 基础使用（四）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/07/2788855167/">小白学 Python 爬虫（15）：urllib 基础使用（五）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/09/1691033431/">小白学 Python 爬虫（16）：urllib 实战之爬取妹子图</a></p>
<p><a href="https://www.geekdigging.com/2019/12/10/1910005577/">小白学 Python 爬虫（17）：Requests 基础使用</a></p>
<p><a href="https://www.geekdigging.com/2019/12/11/1468953802/">小白学 Python 爬虫（18）：Requests 进阶操作</a></p>
<p><a href="https://www.geekdigging.com/2019/12/12/3568648672/">小白学 Python 爬虫（19）：Xpath 基操</a></p>
<p><a href="https://www.geekdigging.com/2019/12/13/2569867940/">小白学 Python 爬虫（20）：Xpath 进阶</a></p>
<p><a href="https://www.geekdigging.com/2019/12/15/2789385418/">小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/16/876770087/">小白学 Python 爬虫（22）：解析库 Beautiful Soup（下）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/17/876770088/">小白学 Python 爬虫（23）：解析库 pyquery 入门</a></p>
<p><a href="https://www.geekdigging.com/2019/12/18/1275791678/">小白学 Python 爬虫（24）：2019 豆瓣电影排行</a></p>
<h2 id="引言">引言</h2>
<p>上一篇的实战写到最后没有用到页面元素解析，感觉有点小遗憾，不过最后的电影列表还是挺香的，真的推荐一看。</p>
<p><img src="./images/小白学 Python 爬虫（25）：爬取股票信息1.png" /></p>
<p>本次选题是先写好代码再写的文章，绝对可以用到页面元素解析，并且还需要对网站的数据加载有一定的分析，才能得到最终的数据，并且小编找的这两个数据源无 ip 访问限制，质量有保证，绝对是小白练手的绝佳之选。</p>
<p><strong>郑重声明：</strong> 本文仅用于学习等目的。</p>
<h2 id="分析">分析</h2>
<p>首先要爬取股票数据，肯定要先知道有哪些股票吧，这里小编找到了一个网站，这个网站上有股票的编码列表：<a href="https://hq.gucheng.com/gpdmylb.html" class="uri">https://hq.gucheng.com/gpdmylb.html</a> 。</p>
<p><img src="./images/小白学 Python 爬虫（25）：爬取股票信息2.png" /></p>
<p>打开 Chrome 的开发者模式，将股票代码一个一个选出来吧。具体过程小编就不贴了，各位同学自行实现。</p>
<p>我们可以将所有的股票代码存放在一个列表中，剩下的就是找一个网站，循环的去将每一只股票的数据取出来咯。</p>
<p>这个网站小编已经找好了，是同花顺，链接： <a href="http://stockpage.10jqka.com.cn/000001/" class="uri">http://stockpage.10jqka.com.cn/000001/</a> 。</p>
<p><img src="./images/小白学 Python 爬虫（25）：爬取股票信息3.png" /></p>
<p>想必各位聪明的同学已经发现了，这个链接中的 <code>000001</code> 就是股票代码。</p>
<p>我们接下来只需要拼接这个链接，就能源源不断的获取到我们想要的数据。</p>
<h2 id="实战">实战</h2>
<p>首先，还是先介绍一下本次实战用到的请求库和解析库为： Requests 和 pyquery 。数据存储最后还是落地在 Mysql 。</p>
<h3 id="获取股票代码列表">获取股票代码列表</h3>
<p>第一步当然是先构建股票代码列表咯，我们先定义一个方法：</p>
<pre><code><code>def get_stock_list(stockListURL):
    r =requests.get(stockListURL, headers = headers)
    doc = PyQuery(r.text)
    list = []
    # 获取所有 section 中 a 节点，并进行迭代
    for i in doc(&#39;.stockTable a&#39;).items():
        try:
            href = i.attr.href
            list.append(re.findall(r&quot;\d{6}&quot;, href)[0])
        except:
            continue
    list = [item.lower() for item in list]  # 将爬取信息转换小写
    return list</code></pre>
<p>将上面的链接当做参数传入，大家可以自己运行下看下结果，小编这里就不贴结果了，有点长。。。</p>
<h3 id="获取详情数据">获取详情数据</h3>
<p>详情的数据看起来好像是在页面上的，但是，实际上并不在，实际最终获取数据的地方并不是页面，而是一个数据接口。</p>
<pre><code><code>http://qd.10jqka.com.cn/quote.php?cate=real&amp;type=stock&amp;callback=showStockDate&amp;return=json&amp;code=000001</code></pre>
<p>至于是怎么找出来，小编这次就不说，还是希望各位想学爬虫的同学能自己动动手，去寻找一下，多找几次，自然就摸到门路了。</p>
<p>现在数据接口有了，我们先看下返回的数据吧：</p>
<pre><code><code>showStockDate({&quot;info&quot;:{&quot;000001&quot;:{&quot;name&quot;:&quot;\u5e73\u5b89\u94f6\u884c&quot;}},&quot;data&quot;:{&quot;000001&quot;:{&quot;10&quot;:&quot;16.13&quot;,&quot;8&quot;:&quot;16.14&quot;,&quot;9&quot;:&quot;15.87&quot;,&quot;13&quot;:&quot;78795234.00&quot;,&quot;19&quot;:&quot;1262802470.00&quot;,&quot;7&quot;:&quot;16.12&quot;,&quot;15&quot;:&quot;40225508.00&quot;,&quot;14&quot;:&quot;37528826.00&quot;,&quot;69&quot;:&quot;17.73&quot;,&quot;70&quot;:&quot;14.51&quot;,&quot;12&quot;:&quot;5&quot;,&quot;17&quot;:&quot;945400.00&quot;,&quot;264648&quot;:&quot;0.010&quot;,&quot;199112&quot;:&quot;0.062&quot;,&quot;1968584&quot;:&quot;0.406&quot;,&quot;2034120&quot;:&quot;9.939&quot;,&quot;1378761&quot;:&quot;16.026&quot;,&quot;526792&quot;:&quot;1.675&quot;,&quot;395720&quot;:&quot;-948073.000&quot;,&quot;461256&quot;:&quot;-39.763&quot;,&quot;3475914&quot;:&quot;313014790000.000&quot;,&quot;1771976&quot;:&quot;1.100&quot;,&quot;6&quot;:&quot;16.12&quot;,&quot;11&quot;:&quot;&quot;}}})</code></pre>
<p>很明显，这个结果并不是标准的 json 数据，但这个是 JSONP 返回的标准格式的数据，这里我们先处理下头尾，将它变成一个标准的 json 数据，再对照这页面的数据进行解析，最后将分析好的值写入数据库中。</p>
<pre><code><code>def getStockInfo(list, stockInfoURL):
    count = 0
    for stock in list:
        try:
            url = stockInfoURL + stock
            r = requests.get(url, headers=headers)
            # 将获取到的数据封装进字典
            dict1 = json.loads(r.text[14: int(len(r.text)) - 1])
            print(dict1)

            # 获取字典中的数据构建写入数据模版
            insert_data = {
                &quot;code&quot;: stock,
                &quot;name&quot;: dict1[&#39;info&#39;][stock][&#39;name&#39;],
                &quot;jinkai&quot;: dict1[&#39;data&#39;][stock][&#39;7&#39;],
                &quot;chengjiaoliang&quot;: dict1[&#39;data&#39;][stock][&#39;13&#39;],
                &quot;zhenfu&quot;: dict1[&#39;data&#39;][stock][&#39;526792&#39;],
                &quot;zuigao&quot;: dict1[&#39;data&#39;][stock][&#39;8&#39;],
                &quot;chengjiaoe&quot;: dict1[&#39;data&#39;][stock][&#39;19&#39;],
                &quot;huanshou&quot;: dict1[&#39;data&#39;][stock][&#39;1968584&#39;],
                &quot;zuidi&quot;: dict1[&#39;data&#39;][stock][&#39;9&#39;],
                &quot;zuoshou&quot;: dict1[&#39;data&#39;][stock][&#39;6&#39;],
                &quot;liutongshizhi&quot;: dict1[&#39;data&#39;][stock][&#39;3475914&#39;]
            }
            cursor.execute(sql_insert, insert_data)
            conn.commit()
            print(stock, &#39;：写入完成&#39;)
        except:
            print(&#39;写入异常&#39;)
            # 遇到错误继续循环
            continue</code></pre>
<p>这里我们加入异常处理，因为本次爬取的数据有些多，很有可能由于某些原因抛出异常，我们当然不希望有异常的时候中断数据抓取，所以这里添加异常处理继续抓取数据。</p>
<h2 id="完整代码">完整代码</h2>
<p>我们将代码稍作封装，完成本次的实战。</p>
<pre><code><code>import requests
import re
import json
from pyquery import PyQuery
import pymysql

# 数据库连接
def connect():
    conn = pymysql.connect(host=&#39;localhost&#39;,
                           port=3306,
                           user=&#39;root&#39;,
                           password=&#39;password&#39;,
                           database=&#39;test&#39;,
                           charset=&#39;utf8mb4&#39;)

    # 获取操作游标
    cursor = conn.cursor()
    return {&quot;conn&quot;: conn, &quot;cursor&quot;: cursor}

connection = connect()
conn, cursor = connection[&#39;conn&#39;], connection[&#39;cursor&#39;]

sql_insert = &quot;insert into stock(code, name, jinkai, chengjiaoliang, zhenfu, zuigao, chengjiaoe, huanshou, zuidi, zuoshou, liutongshizhi, create_date) values (%(code)s, %(name)s, %(jinkai)s, %(chengjiaoliang)s, %(zhenfu)s, %(zuigao)s, %(chengjiaoe)s, %(huanshou)s, %(zuidi)s, %(zuoshou)s, %(liutongshizhi)s, now())&quot;

headers = {
    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&#39;
}

def get_stock_list(stockListURL):
    r =requests.get(stockListURL, headers = headers)
    doc = PyQuery(r.text)
    list = []
    # 获取所有 section 中 a 节点，并进行迭代
    for i in doc(&#39;.stockTable a&#39;).items():
        try:
            href = i.attr.href
            list.append(re.findall(r&quot;\d{6}&quot;, href)[0])
        except:
            continue
    list = [item.lower() for item in list]  # 将爬取信息转换小写
    return list


def getStockInfo(list, stockInfoURL):
    count = 0
    for stock in list:
        try:
            url = stockInfoURL + stock
            r = requests.get(url, headers=headers)
            # 将获取到的数据封装进字典
            dict1 = json.loads(r.text[14: int(len(r.text)) - 1])
            print(dict1)

            # 获取字典中的数据构建写入数据模版
            insert_data = {
                &quot;code&quot;: stock,
                &quot;name&quot;: dict1[&#39;info&#39;][stock][&#39;name&#39;],
                &quot;jinkai&quot;: dict1[&#39;data&#39;][stock][&#39;7&#39;],
                &quot;chengjiaoliang&quot;: dict1[&#39;data&#39;][stock][&#39;13&#39;],
                &quot;zhenfu&quot;: dict1[&#39;data&#39;][stock][&#39;526792&#39;],
                &quot;zuigao&quot;: dict1[&#39;data&#39;][stock][&#39;8&#39;],
                &quot;chengjiaoe&quot;: dict1[&#39;data&#39;][stock][&#39;19&#39;],
                &quot;huanshou&quot;: dict1[&#39;data&#39;][stock][&#39;1968584&#39;],
                &quot;zuidi&quot;: dict1[&#39;data&#39;][stock][&#39;9&#39;],
                &quot;zuoshou&quot;: dict1[&#39;data&#39;][stock][&#39;6&#39;],
                &quot;liutongshizhi&quot;: dict1[&#39;data&#39;][stock][&#39;3475914&#39;]
            }
            cursor.execute(sql_insert, insert_data)
            conn.commit()
            print(stock, &#39;：写入完成&#39;)
        except:
            print(&#39;写入异常&#39;)
            # 遇到错误继续循环
            continue
def main():
    stock_list_url = &#39;https://hq.gucheng.com/gpdmylb.html&#39;
    stock_info_url = &#39;http://qd.10jqka.com.cn/quote.php?cate=real&amp;type=stock&amp;callback=showStockDate&amp;return=json&amp;code=&#39;
    list = get_stock_list(stock_list_url)
    # list = [&#39;601766&#39;]
    getStockInfo(list, stock_info_url)

if __name__ == &#39;__main__&#39;:
    main()</code></pre>
<h2 id="成果">成果</h2>
<p>最终小编耗时 15 分钟左右，成功抓取数据 4600+ 条，结果就不展示了。</p>
<h2 id="示例代码">示例代码</h2>
<p>本系列的所有代码小编都会放在代码管理仓库 Github 和 Gitee 上，方便大家取用。</p>
<p><a href="https://github.com/meteor1993/python-learning/tree/master/python-spider/gupiao-demo" title="示例代码-Github">示例代码-Github</a></p>
<p><a href="https://gitee.com/inwsy/python-learning/tree/master/python-spider/gupiao-demo" title="示例代码-Gitee">示例代码-Gitee</a></p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>