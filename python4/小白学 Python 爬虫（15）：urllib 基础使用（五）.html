<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修小白学 Python 爬虫（15）：urllib 基础使用（五）' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>小白学 Python 爬虫（15）：urllib 基础使用（五）</center></div><div class='banquan'>原文出处:本文由博客园博主极客挖掘机提供。<br/>
原文连接:https://www.cnblogs.com/babycomeon/p/12020258.html</div><br>
    <p><img src="./images/小白学 Python 爬虫（15）：urllib 基础使用（五）0.png" /></p>
<blockquote>
<p>人生苦短，我用 Python</p>
</blockquote>
<p>前文传送门：</p>
<p><a href="https://www.geekdigging.com/2019/11/13/3303836941/">小白学 Python 爬虫（1）：开篇</a></p>
<p><a href="https://www.geekdigging.com/2019/11/20/2586166930/">小白学 Python 爬虫（2）：前置准备（一）基本类库的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/21/1005563697/">小白学 Python 爬虫（3）：前置准备（二）Linux基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/22/3679472340/">小白学 Python 爬虫（4）：前置准备（三）Docker基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/24/334078215/">小白学 Python 爬虫（5）：前置准备（四）数据库基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/25/1881661601/">小白学 Python 爬虫（6）：前置准备（五）爬虫框架的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/26/1197821400/">小白学 Python 爬虫（7）：HTTP 基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/27/101847406/">小白学 Python 爬虫（8）：网页基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/28/1668465912/">小白学 Python 爬虫（9）：爬虫基础</a></p>
<p><a href="https://www.geekdigging.com/2019/12/01/2475257648/">小白学 Python 爬虫（10）：Session 和 Cookies</a></p>
<p><a href="https://www.geekdigging.com/2019/12/02/2333822325/">小白学 Python 爬虫（11）：urllib 基础使用（一）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/03/819896244/">小白学 Python 爬虫（12）：urllib 基础使用（二）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/04/2992515886/">小白学 Python 爬虫（13）：urllib 基础使用（三）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/05/104488944/">小白学 Python 爬虫（14）：urllib 基础使用（四）</a></p>
<h2 id="引言">引言</h2>
<p>前面几篇 urllib 的基础介绍，分别介绍了 urllib 的处理 URL 的模块，还剩最后一个 robotparser 模块未做介绍，本篇文章来简单的聊聊 robotparser 。</p>
<p>Robotparser 从命名上来看，好像是说机器人，实际上这里的机器人指的是爬虫。</p>
<p>在说 Robotparser 之前，我们先介绍一个概念：Robots协议。</p>
<h2 id="robots-协议">Robots 协议</h2>
<p>Robots 协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol）。</p>
<p>Robots 协议通常会保存在一个叫做 <code>robots.txt</code> 的文本文件中，该文件一般位于网站的跟目录中。</p>
<p>这个文件中记载了该网站哪些目录允许爬取，哪些目录不允许爬取。</p>
<p>虽然这个文件并不是强制生效的，但是各位同学最好可以遵循此文件定义的爬取规则。</p>
<p>最近很多搞爬虫的公司都进去了，各位同学一定引以为戒，做一位知法守法的好公民。</p>
<p>我们来看一下淘宝的 Robots 协议，一起了解下 Robots 协议的语法规则：</p>
<p>以下 <code>robots.txt</code> 内容来源：<a href="https://www.taobao.com/robots.txt" class="uri">https://www.taobao.com/robots.txt</a> ，由于内容过多，仅截取部分内容。</p>
<pre><code><code>User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Allow:  /ershou
Allow: /$
Disallow:  /product/
Disallow:  /

User-Agent:  Googlebot
Allow:  /article
Allow:  /oshtml
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Allow:  /ershou
Allow: /$
Disallow:  /

......</code></pre>
<p>可以看到，一个 <code>robots.txt</code> 总共分为三个部分：</p>
<ul>
<li>User-Agent：描述了搜索爬虫的名称，如果设置为 * 则代表对所有爬虫生效。</li>
<li>Allow：指定了云讯爬取的目录。</li>
<li>Disallow：指定了不允许爬取的目录，一般和 Allow 配合使用。</li>
</ul>
<p>比如上面的这个 Robots 协议，其中的内容定义了百度爬虫和谷歌爬虫的爬取规则，其中对百度爬虫定义了的可爬取路径有 <code>/article</code> 、 <code>/oshtml</code> 、 <code>/ershou</code> 、 <code>/$</code> ，不可爬取的路径有 <code>/product/</code> 和 <code>/</code> 目录。</p>
<p>各位同学可能会有疑问，爬虫的名字是哪来的呢？</p>
<p>emmmmmmmmm，这个小编也不清楚，就当做一些固定用法吧，下表列出一些常见的爬虫名称：</p>
<table>
<thead>
<tr class="header">
<th>爬虫名称</th>
<th>来源</th>
<th>来源网站</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BaiduSpider</td>
<td>百度搜索</td>
<td>www.baidu.com</td>
</tr>
<tr class="even">
<td>Googlebot</td>
<td>谷歌搜索</td>
<td>www.google.com</td>
</tr>
<tr class="odd">
<td>360Spider</td>
<td>360 搜索</td>
<td>www.so.com</td>
</tr>
<tr class="even">
<td>Bingbot</td>
<td>必应搜索</td>
<td>cn.bing.com</td>
</tr>
<tr class="odd">
<td>Yisouspider</td>
<td>神马搜索</td>
<td>m.sm.cn</td>
</tr>
<tr class="even">
<td>Sogouspider</td>
<td>搜狗搜索</td>
<td>www.sogou.com</td>
</tr>
<tr class="odd">
<td>Yahoo! Slurp</td>
<td>雅虎搜索</td>
<td>www.yahoo.com</td>
</tr>
<tr class="even">
<td>Sosospider</td>
<td>搜搜</td>
<td>www.soso.com</td>
</tr>
</tbody>
</table>
<h2 id="robotparser">robotparser</h2>
<p>官方文档：<a href="https://docs.python.org/zh-cn/3.7/library/urllib.robotparser.html" class="uri">https://docs.python.org/zh-cn/3.7/library/urllib.robotparser.html</a></p>
<p>在了解了什么是 Robots 协议之后，我们可以使用 robotparser 来解析 Robots 协议。</p>
<p>该模块提供了一个类 <code>RobotFileParser</code> 它可以根据某网站的 <code>robots.txt</code> 文件来判断一个爬取爬虫是否有权限来爬取这个网站的某个路径。</p>
<p>首先我们看一下这个类的声明：</p>
<pre><code><code>urllib.robotparser.RobotFileParser(url=&#39;&#39;)</code></pre>
<p>看起来很简单，只需要在构造方法中传入 <code>robots.txt</code> 的路径即可。</p>
<p>接下来我们来看下这个类所具有的方法：</p>
<ul>
<li>set_url(url)： 如果在声明 <code>RobotFileParser</code> 的时候没有传入 <code>robots.txt</code> 的路径，可以调用这个方法传入 <code>robots.txt</code> 的路径。</li>
<li>read()：读取 <code>robots.txt</code> 文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False ，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。</li>
<li>parse(lines)：用来解析 <code>robots.txt</code> 文件，传入的参数是 <code>robots.txt</code> 某些行的内容，它会按照 <code>robots.txt</code> 的语法规则来分析这些内容。</li>
<li>can_fetch(useragent, url)：该方法传入两个参数，第一个是 User-agent ，第二个是要抓取的 URL 。返回的内容是该搜索引擎是否可以抓取这个 URL ，返回结果是 True 或 False 。</li>
<li>mtime()：返回的是上次抓取和分析 <code>robots.txt</code> 的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 <code>robots.txt</code> 。</li>
<li>modified()：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 <code>robots.txt</code> 的时间。</li>
<li>crawl_delay(useragent)：从 <code>robots.txt</code> 返回有关用户代理的抓取延迟参数的值。 如果没有这样的参数，或者该参数不适用于指定的用户代理，或者该参数的 <code>robots.txt</code> 条目的语法无效，则返回 None 。</li>
<li>request_rate(useragent)：以指定的元组 RequestRate（requests，seconds） 的形式从 <code>robots.txt</code> 返回 Request-rate 参数的内容。 如果没有这样的参数，或者该参数不适用于指定的用户代理，或者该参数的 <code>robots.txt</code> 条目的语法无效，则返回 None 。</li>
</ul>
<p>举一个简单的例子：</p>
<pre><code><code>import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url(&quot;https://www.taobao.com/robots.txt&quot;)
rp.read()

print(rp.can_fetch(&#39;Googlebot&#39;, &#39;https://www.taobao.com/article&#39;))
print(rp.can_fetch(&#39;Googlebot&#39;, &quot;https://s.taobao.com/search?initiative_id=tbindexz_20170306&amp;ie=utf8&amp;spm=a21bo.2017.201856-taobao-item.2&amp;sourceId=tb.index&amp;search_type=item&amp;ssid=s5-e&amp;commend=all&amp;imgfile=&amp;q=iphone&amp;suggest=history_1&amp;_input_charset=utf-8&amp;wq=&amp;suggest_query=&amp;source=suggest&quot;))</code></pre>
<p>执行结果如下：</p>
<pre><code><code>True
False</code></pre>
<p>小编这里就用某宝做栗子了，首先创建 RobotFileParser 对象，然后通过 set_url() 方法设置了 <code>robots.txt</code> 的链接。</p>
<p>当然，不用这个方法的话，可以在声明时直接用如下方法设置：</p>
<pre><code><code>rp = urllib.robotparser.RobotFileParser(&#39;https://www.taobao.com/robots.txt&#39;)</code></pre>
<p>接着利用 can_fetch() 方法判断了网页是否可以被抓取。</p>
<h2 id="小结">小结</h2>
<p>本篇内容到这里就结束了，内容比较简单，希望各位同学以后在写爬虫时候可以遵循 Robot 协议，为了自己的安全着想。</p>
<p>希望各位同学可以自己动手实际操作试试看。</p>
<h2 id="示例代码">示例代码</h2>
<p>本系列的所有代码小编都会放在代码管理仓库 Github 和 Gitee 上，方便大家取用。</p>
<p><a href="https://github.com/meteor1993/python-learning/tree/master/python-spider/urllib-request" title="示例代码-Github">示例代码-Github</a></p>
<p><a href="https://gitee.com/inwsy/python-learning/tree/master/python-spider/urllib-request" title="示例代码-Gitee">示例代码-Gitee</a></p>
<h2 id="参考">参考</h2>
<p><a href="https://www.cnblogs.com/zhangxinqi/p/9170312.html" class="uri">https://www.cnblogs.com/zhangxinqi/p/9170312.html</a></p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>