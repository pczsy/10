<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)</center></div><div class='banquan'>原文出处:本文由博客园博主九命猫幺提供。<br/>
原文连接:https://www.cnblogs.com/yongestcat/p/11456957.html</div><br>
    <p>学习目的：<br>掌握定向网络数据爬取和网页解析的基本能力<br>the Website is the API…<h2>1 python ide</h2><p>文本ide：IDLE,Sublime&nbsp;&nbsp;&nbsp; Text<br>集成ide：Pycharm,Anaconda&amp;Spyder,Wing,Visual Studio &amp; PTVS,Eclipse &amp; PyDev,Canopy<p>默认源太慢：<br>阿里云 http://mirrors.aliyun.com/pypi/simple/<br>中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/<br>豆瓣(douban) https://pypi.douban.com/simple/<br>清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/<br>中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple/<h2>2 网络爬虫规则</h2><h3>2.1 Requests库 自动爬取html页面</h3><p>#安装方法 管理员权限启动cmd安装<br>pip install requests -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><p>#测试下<img width="1641" height="918" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)0.png" border="0"><p>#requests库7个主要方法：<br>a、requests.request() 构造一个请求，支撑以下各方法的基础方法<br>b、requests.get() 获取HTML网页的主要方法，对应于HTTP的GET<br>c、requests.head() 获取HTML网页头信息的方法，对应于HTTP的HEAD<br>d、requests.post() 向HTML网页提交POST请求的方法，对应于HTTP的POST<br>e、requests.put() 向HTML网页提交PUT请求的方法，对应于HTTP的PUT<br>f、requests.patch() 向HTML网页提交局部修改请求，对应于HTTP的PATCH<br>g、requests.delete() 向HTML页面提交删除请求，对应于HTTP的DELETE<p>r = requests.get(url)&nbsp;&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r是返回一个包含服务器资源的Response对象，右边是构造一个向服务器请求资源的Request对象<br>requests.get(url,params=None,**kwargs)&nbsp; 完整格式<br>&nbsp;&nbsp;&nbsp; params:url中的额外参数，字典或者字节流格式可选<br>&nbsp;&nbsp;&nbsp; **kwargs：12个控制访问参数可选<br>打开源码可知，get方法是调用requests方法封装的，实际上7个方法中，其余6个都是由request方法封装<br>Response对象常用5个属性<br>&nbsp;&nbsp;&nbsp; r.status_code HTTP请求的返回状态，200表示连接成功，404表示失败<br>&nbsp;&nbsp;&nbsp; r.text HTTP响应内容的字符串形式，即，url对应的页面内容<br>&nbsp;&nbsp;&nbsp; r.encoding 从HTTP header中猜测的响应内容编码方式，如果header中不存在charset,则认为编码为ISO-8859-1<br>&nbsp;&nbsp;&nbsp; r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式）<br>&nbsp;&nbsp;&nbsp; r.content HTTP响应内容的二进制形式<br>requests库异常<br>&nbsp; requests.ConnectionError 网络连接错误异常，如DNS查询失败、拒绝连接等<br>&nbsp;&nbsp;&nbsp; requests.HTTPError HTTP错误异常<br>&nbsp;&nbsp;&nbsp; requests.URLRequired URL缺失异常<br>&nbsp;&nbsp;&nbsp; requests.TooManyRedirects 超过最大重定向次数，产生重定向异常<br>&nbsp;&nbsp;&nbsp; requests.ConnectTimeout 连接远程服务器超时异常<br>&nbsp;&nbsp;&nbsp; requests.Timeout 请求URL超时，产生超时异常<br>爬取网页的通用代码框架<br>import requests<br>def getHTMLText(url):<br>&nbsp;&nbsp;&nbsp; try:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r = requests.get(url,timeout=30)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r.raise_for_status()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r.encoding = r.apparent_encoding<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return r.text<br>&nbsp;&nbsp;&nbsp; except:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return “产生异常”<br>if __name__ == “__main__”:<br>&nbsp;&nbsp;&nbsp; url = “<a href="http://www.baidu.com%22/">http://www.baidu.com”</a><br>&nbsp;&nbsp;&nbsp; print(getHTMLText(url))<p>http协议对资源的6中操作：&nbsp;&nbsp; <br>GET 请求获取URL位置的资源<br>HEAD 请求获取URL位置资源的响应消息报告，即获得该资源的头部信息<br>POST 请求向URL位置的资源后附加新的数据<br>PUT 请求向URL位置存储一个资源，覆盖原URL位置的资源<br>PATCH 请求局部更新URL位置的资源，即改变该处资源的部分内容<br>DELETE 请求删除URL位置存储的资源&nbsp; <p>通过URL和命令管理资源，操作独立无状态，网络通道及服务器成为了黑盒子<p>理解PATCH和PUT的区别：<br>假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段<br>需求：用户修改了UserName，其他不变<br>. 采用PATCH，仅向URL提交UserName的局部更新请求<br>. 采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除<br>PATCH的最主要好处：节省网络带宽<p>http协议与requests库功能是一致的<p>requests.request()<br>requests.request(method, url, **kwargs)<br>method : 请求方式，对应get/put/post等7种<br>∙ url : 拟获取页面的url链接<br>∙ **kwargs: 控制访问的参数，共13个<p>method : 请求方式<br>r = requests.request(‘GET’, url, **kwargs)<br>r = requests.request(‘HEAD’, url, **kwargs)<br>r = requests.request(‘POST’, url, **kwargs)<br>r = requests.request(‘PUT’, url, **kwargs)<br>r = requests.request(‘PATCH’, url, **kwargs)<br>r = requests.request(‘delete’, url, **kwargs)<br>r = requests.request(‘OPTIONS’, url, **kwargs)<p>**kwargs: 控制访问的参数，均为可选项<br>params : 字典或字节序列，作为参数增加到url中<br>data : 字典、字节序列或文件对象，作为Request的内容<br>json : JSON格式的数据，作为Request的内容<br>headers : 字典，HTTP定制头<br>cookies : 字典或CookieJar，Request中的cookie<br>auth : 元组，支持HTTP认证功能<br>files : 字典类型，传输文件<br>timeout : 设定超时时间，秒为单位<br>proxies : 字典类型，设定访问代理服务器，可以增加登录认证<br>allow_redirects : True/False，默认为True，重定向开关<br>stream : True/False，默认为True，获取内容立即下载开关<br>verify : True/False，默认为True，认证SSL证书开关<br>cert : 本地SSL证书路径<h3>2.2 robots.txt 网络爬虫排除标准</h3><p>小规模，数据量小，爬取速度不敏感，Requests库 ， 90%以上&nbsp;&nbsp; ，&nbsp;&nbsp; 爬取网页 玩转网页<br>中规模，数据规模较大，爬取速度敏感，Scrapy库 ，爬取网站 爬取系列网站<br>大规模，搜索引擎爬取，速度关键 ，定制开发，爬取全网<p>限制网络爬虫：1.来源审查 2.robots协议<h3>2.3 实战项目</h3><p>a.爬取京东某网页<img width="1284" height="461" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)1.png" border="0">b、爬取亚马逊某网页 有来源审查防爬虫<img width="953" height="458" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)2.png" border="0">c.爬取百度搜索关键词<img width="959" height="356" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)3.png" border="0">d.网络图片的爬取和存储<img width="935" height="449" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)4.png" border="0"><h2>3 网络爬虫规则之提取</h2><h3>3.1 Beautiful Soup库入门</h3><p>#安装<p>pip install beautifulsoup4<img width="1297" height="390" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)5.png" border="0">#测试下<img width="949" height="646" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)6.png" border="0">html文档&nbsp; 等价于&nbsp; 标签树&nbsp; 等价于 BeautifulSoup类<img width="959" height="118" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)7.png" border="0"><p>Beautiful Soup库，也叫beautifulsoup4 或bs4<br>约定引用方式如下，即主要是用BeautifulSoup类<br>import bs4 from<br>import bs4 from BeautifulSoup<p>4种解析器：<br>soup = BeautifulSoup(‘&lt;html&gt;data&lt;/html&gt;’，’html.parser’)<br>bs4的HTML解析器&nbsp;&nbsp; BeautifulSoup(mk,’html.parser’)&nbsp;&nbsp;&nbsp;&nbsp; 安装bs4库<br>lxml的HTML解析器&nbsp;&nbsp;&nbsp; BeautifulSoup(mk,’lxml’)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pip install lxml<br>lxml的XML解析器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BeautifulSoup(mk,’xml’)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pip install lxml<br>html5lib的解析器&nbsp;&nbsp;&nbsp; BeautifulSoup(mk,’html5lib’)&nbsp;&nbsp;&nbsp;&nbsp; pip install html5lib<p>BeautifulSoup类5种基本元素：<br>Tag 标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾<br>Name 标签的名字，&lt;p&gt;…&lt;/p&gt;的名字是’p’，格式：&lt;tag&gt;.name<br>Attributes 标签的属性，字典形式组织，格式：&lt;tag&gt;.attrs<br>NavigableString 标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串，格式：&lt;tag&gt;.string<br>Comment 标签内字符串的注释部分，一种特殊的Comment类型<p>Tag 标签：<br>任何存在于HTML语法中的标签都可以用soup.&lt;tag&gt;访问获得<br>当HTML文档中存在多个相同&lt;tag&gt;对应内容时，soup.&lt;tag&gt;返回第一个<img width="962" height="192" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)8.png" border="0">Tag的name（名字）：每个&lt;tag&gt;都有自己的名字，通过&lt;tag&gt;.name获取，字符串类型<img width="960" height="363" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)9.png" border="0">Tag的attrs（属性）：一个&lt;tag&gt;可以有0或多个属性，字典类型<img width="968" height="375" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)10.png" border="0">Tag的NavigableString：NavigableString可以跨越多个层次<img width="963" height="369" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)11.png" border="0">Tag的Comment：Comment是一种特殊类型<img width="968" height="333" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)12.png" border="0"><p>html或者xml都是树形结构<p>三种遍历方式：下行遍历、下行遍历、平行遍历<p>BeautifulSoup类型是标签树的根节点<br>下行遍历：<br>.contents 子节点的列表，将&lt;tag&gt;所有儿子节点存入列表<br>.children 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点<br>.descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历<br>遍历儿子节点<br>for child in soup.body.children:<br>&nbsp;&nbsp;&nbsp; print(child)<br>遍历子孙节点<br>for child in soup.body.descendants:<br>&nbsp;&nbsp;&nbsp; print(child)<img width="966" height="454" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)13.png" border="0"><p>标签树的上行遍历<br>属性说明<br>.parent 节点的父亲标签<br>.parents 节点先辈标签的迭代类型，用于循环遍历先辈节点<br>遍历所有先辈节点，包括soup本身，所以要区别判断<img width="968" height="865" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)14.png" border="0"><p>标签树的平行遍历&nbsp; 同一个父亲下<br>属性说明<br>.next_sibling 返回按照HTML文本顺序的下一个平行节点标签<br>.previous_sibling 返回按照HTML文本顺序的上一个平行节点标签<br>.next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签<br>.previous_siblings 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签<br>遍历后续节点<br>for sibling in soup.a.next_sibling:<br>&nbsp;&nbsp;&nbsp; print(sibling)<br>遍历前续节点<br>for sibling in soup.a.previous_sibling:<br>&nbsp;&nbsp;&nbsp; print(sibling)<img width="958" height="949" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)15.png" border="0"><p>基于bs4库的HTML格式输出&nbsp; 如何让html页面更加友好的输出<br>bs4库的prettify()方法<br>.prettify()为HTML文本&lt;&gt;及其内容增加更加’\n’<br>.prettify()可用于标签，方法：&lt;tag&gt;.prettify()<br>bs4库的编码<br>bs4库将任何HTML输入都变成utf‐8编码<br>Python 3.x默认支持编码是utf‐8,解析无障碍<h3>3.2 信息组织与提取</h3><p>信息标记的三种形式：xml、json、yaml<br>标记后的信息可形成信息组织结构，增加了信息维度<br>标记的结构与信息一样具有重要价值<br>标记后的信息可用于通信、存储或展示<br>标记后的信息更利于程序理解和运用<p>文本、声音、图像、视频<img width="371" height="836" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)16.png" border="0"><img width="961" height="478" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)17.png" border="0"><img width="959" height="467" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)18.png" border="0"><img width="959" height="711" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)19.png" border="0"><p>XML 最早的通用信息标记语言，可扩展性好，但繁琐&nbsp;&nbsp;&nbsp; Internet上的信息交互与传递<br>JSON 信息有类型，适合程序处理(js)，较XML简洁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 移动应用云端和节点的信息通信，无注释<br>YAML 信息无类型，文本信息比例最高，可读性好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 各类系统的配置文件，有注释易读<p>信息提取的一般方法<br>从标记后的信息中提取所关注的内容xml、json、yaml<br>方法一：完整解析信息的标记形式，再提取关键信息<br>优点：信息解析准确<br>缺点：提取过程繁琐，速度慢<br>方法二：无视标记形式，直接搜索关键信息&nbsp; 对信息的文本查找函数即可<br>优点：提取过程简洁，速度较快<br>缺点：提取结果准确性与信息内容相关<br>融合方法：结合形式解析与搜索方法，提取关键信息<br>需要标记解析器及文本查找函数<p>&lt;&gt;.find_all(name, attrs, recursive, string, **kwargs)<br>返回一个列表类型，存储查找的结果<br>∙ name : 对标签名称的检索字符串<br>∙ attrs: 对标签属性值的检索字符串，可标注属性检索<br>∙ recursive: 是否对子孙全部检索，默认True<br>∙ string: &lt;&gt;…&lt;/&gt;中字符串区域的检索字符串<p>&lt;tag&gt;(..) 等价于&lt;tag&gt;.find_all(..)<br>soup(..) 等价于soup.find_all(..)<p>扩展方法：<p>&lt;&gt;.find() 搜索且只返回一个结果，同.find_all()参数<br>&lt;&gt;.find_parents() 在先辈节点中搜索，返回列表类型，同.find_all()参数<br>&lt;&gt;.find_parent() 在先辈节点中返回一个结果，同.find()参数<br>&lt;&gt;.find_next_siblings() 在后续平行节点中搜索，返回列表类型，同.find_all()参数<br>&lt;&gt;.find_next_sibling() 在后续平行节点中返回一个结果，同.find()参数<br>&lt;&gt;.find_previous_siblings() 在前序平行节点中搜索，返回列表类型，同.find_all()参数<br>&lt;&gt;.find_previous_sibling() 在前序平行节点中返回一个结果，同.find()参数<img width="970" height="479" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)20.png" border="0"><h3>3.3 实例：大学排名爬取</h3><div class="cnblogs_Highlighter">
<pre><code>#CrawUnivRankingB.py
import requests
from bs4 import BeautifulSoup
import bs4
 
def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
 
def fillUnivList(ulist, html):
    soup = BeautifulSoup(html, "html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr, bs4.element.Tag):
            tds = tr('td')
            ulist.append([tds[0].string, tds[1].string, tds[3].string])
 
def printUnivList(ulist, num):
    tplt = "{0:^10}\t{1:{3}^10}\t{2:^10}"
    print(tplt.format("排名","学校名称","总分",chr(12288)))
    for i in range(num):
        u=ulist[i]
        print(tplt.format(u[0],u[1],u[2],chr(12288)))
     
def main():
    uinfo = []
    url = 'https://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
    html = getHTMLText(url)
    fillUnivList(uinfo, html)
    printUnivList(uinfo, 20) # 20 univs
main()
</pre>
</div>
<h2>4 网络爬虫之实战</h2><h3>4.1正则表达式</h3><p>正则表达式语法由字符和操作符构成<br>常用操作符<br>. 表示任何单个字符<br>[ ] 字符集，对单个字符给出取值范围[abc]表示a、b、c，[a‐z]表示a到z单个字符<br>[^ ] 非字符集，对单个字符给出排除范围[^abc]表示非a或b或c的单个字符<br>* 前一个字符0次或无限次扩展abc* 表示ab、abc、abcc、abccc等<br>+ 前一个字符1次或无限次扩展abc+ 表示abc、abcc、abccc等<br>? 前一个字符0次或1次扩展abc? 表示ab、abc<br>| 左右表达式任意一个abc|def 表示abc、def<br>{m} 扩展前一个字符m次ab{2}c表示abbc<br>{m,n} 扩展前一个字符m至n次（含n） ab{1,2}c表示abc、abbc<br>^ 匹配字符串开头^abc表示abc且在一个字符串的开头<br>$ 匹配字符串结尾abc$表示abc且在一个字符串的结尾<br>( ) 分组标记，内部只能使用| 操作符(abc)表示abc，(abc|def)表示abc、def<br>\d 数字，等价于[0‐9]<br>\w 单词字符，等价于[A‐Za‐z0‐9_]<p>经典正则表达式<br>^[A‐Za‐z]+$&nbsp; 由26个字母组成的字符串<br>^[A‐Za‐z0‐9]+$&nbsp;&nbsp;&nbsp; 由26个字母和数字组成的字符串<br>^‐?\d+$&nbsp;&nbsp;&nbsp; 整数形式的字符串<br>^[0‐9]*[1‐9][0‐9]*$&nbsp;&nbsp;&nbsp; 正整数形式的字符串<br>[1‐9]\d{5}&nbsp;&nbsp;&nbsp; 中国境内邮政编码，6位<br>[\u4e00‐\u9fa5]&nbsp;&nbsp;&nbsp; 匹配中文字符<br>\d{3}‐\d{8}|\d{4}‐\d{7}&nbsp;&nbsp;&nbsp; 国内电话号码，010‐68913536<p>ip地址<br>精确写法0‐99： [1‐9]?\d<br>100‐199: 1\d{2}<br>200‐249: 2[0‐4]\d<br>250‐255: 25[0‐5]<br>(([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5]).){3}([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])<p>raw string类型（原生字符串类型）<br>re库采用raw string类型表示正则表达式，表示为：r’text’ 例如： r'[1‐9]\d{5}’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r’\d{3}‐\d{8}|\d{4}‐\d{7}’<br>raw string是不包含对转义符再次转义的字符串<br>re库也可以采用string类型表示正则表达式，但更繁琐<br>例如：<br>‘[1‐9]\\d{5}’<br>‘\\d{3}‐\\d{8}|\\d{4}‐\\d{7}’<br>建议：当正则表达式包含转义符时，使用raw string<p>re库主要功能函数<br>re.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象<br>re.match() 从一个字符串的开始位置起匹配正则表达式，返回match对象<br>re.findall() 搜索字符串，以列表类型返回全部能匹配的子串<br>re.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型<br>re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象<br>re.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串<p>re.search(pattern, string, flags=0)<br>∙ pattern : 正则表达式的字符串或原生字符串表示<br>∙ string : 待匹配字符串<br>∙ flags : 正则表达式使用时的控制标记<br>常用标记<br>re.I re.IGNORECASE 忽略正则表达式的大小写，[A‐Z]能够匹配小写字符<br>re.M re.MULTILINE 正则表达式中的^操作符能够将给定字符串的每行当作匹配开始<br>re.S re.DOTALL 正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符<img width="975" height="505" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)21.png" border="0"><p>re.match(pattern, string, flags=0)<br>. pattern : 正则表达式的字符串或原生字符串表示<br>. string : 待匹配字符串<br>. flags : 正则表达式使用时的控制标记<img width="973" height="654" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)22.png" border="0"><p>re.findall(pattern, string, flags=0)<br>. pattern : 正则表达式的字符串或原生字符串表示<br>. string : 待匹配字符串<br>. flags : 正则表达式使用时的控制标记<img width="967" height="385" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)23.png" border="0"><p>re.split(pattern, string, maxsplit=0, flags=0)<br>. pattern : 正则表达式的字符串或原生字符串表示<br>. string : 待匹配字符串<br>. maxsplit: 最大分割数，剩余部分作为最后一个元素输出<br>. flags : 正则表达式使用时的控制标记<img width="964" height="343" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)24.png" border="0"><p>re.finditer(pattern, string, flags=0)<br>∙ pattern : 正则表达式的字符串或原生字符串表示<br>∙ string : 待匹配字符串<br>∙ flags : 正则表达式使用时的控制标记<img width="974" height="489" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)25.png" border="0"><p>re.sub(pattern, repl, string, count=0, flags=0)<br>∙ pattern : 正则表达式的字符串或原生字符串表示<br>∙ repl : 替换匹配字符串的字符串<br>∙ string : 待匹配字符串<br>∙ count : 匹配的最大替换次数<br>∙ flags : 正则表达式使用时的控制标记<img width="972" height="332" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)26.png" border="0"><p>regex = re.compile(pattern, flags=0)<br>∙ pattern : 正则表达式的字符串或原生字符串表示<br>∙ flags : 正则表达式使用时的控制标记<br>&gt;&gt;&gt; regex = re.compile(r'[1‐9]\d{5}’)<p>regex.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象<br>regex.match() 从一个字符串的开始位置起匹配正则表达式，返回match对象<br>regex.findall() 搜索字符串，以列表类型返回全部能匹配的子串<br>regex.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型<br>regex.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象<br>regex.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串<p>Match对象是一次匹配的结果，包含匹配的很多信息<br>Match对象的属性：<br>.string 待匹配的文本<br>.re 匹配时使用的patter对象（正则表达式）<br>.pos 正则表达式搜索文本的开始位置<br>.endpos 正则表达式搜索文本的结束位置<br>Match对象的方法：<br>.group(0) 获得匹配后的字符串<br>.start() 匹配字符串在原始字符串的开始位置<br>.end() 匹配字符串在原始字符串的结束位置<br>.span() 返回(.start(), .end())<img width="968" height="616" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)27.png" border="0"><p>Re库的贪婪和最小匹配<br>Re库默认采用贪婪匹配，即输出匹配最长的子串<br>最小匹配操作符:<br>*? 前一个字符0次或无限次扩展，最小匹配<br>+? 前一个字符1次或无限次扩展，最小匹配<br>?? 前一个字符0次或1次扩展，最小匹配<br>{m,n}? 扩展前一个字符m至n次（含n），最小匹配<p>只要长度输出可能不同的，都可以通过在操作符后增加?变成最小匹配<img width="967" height="557" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)28.png" border="0"><h3>4.2 淘宝商品比价定向爬虫</h3><div class="cnblogs_Highlighter">
<pre><code>import requests
import re
 
def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
     
def parsePage(ilt, html):
    try:
        plt = re.findall(r'\"view_price\"\:\"[\d\.]*\"',html)
        tlt = re.findall(r'\"raw_title\"\:\".*?\"',html)
        for i in range(len(plt)):
            price = eval(plt[i].split(':')[1])
            title = eval(tlt[i].split(':')[1])
            ilt.append([price , title])
    except:
        print("")
 
def printGoodsList(ilt):
    tplt = "{:4}\t{:8}\t{:16}"
    print(tplt.format("序号", "价格", "商品名称"))
    count = 0
    for g in ilt:
        count = count + 1
        print(tplt.format(count, g[0], g[1]))
         
def main():
    goods = '书包'
    depth = 3
    start_url = 'https://s.taobao.com/search?q=' + goods
    infoList = []
    for i in range(depth):
        try:
            url = start_url + '&amp;s=' + str(44*i)
            html = getHTMLText(url)
            parsePage(infoList, html)
        except:
            continue
    printGoodsList(infoList)
     
main()
</pre>
</div>
<h3>4.3 股票数据定向爬虫</h3><div class="cnblogs_Highlighter">
<pre><code>import requests
from bs4 import BeautifulSoup
import traceback
import re
 
def getHTMLText(url, code="utf-8"):
    try:
        r = requests.get(url)
        r.raise_for_status()
        r.encoding = code
        return r.text
    except:
        return ""
 
def getStockList(lst, stockURL):
    html = getHTMLText(stockURL, "GB2312")
    soup = BeautifulSoup(html, 'html.parser') 
    a = soup.find_all('a')
    for i in a:
        try:
            href = i.attrs['href']
            lst.append(re.findall(r"[s][hz]\d{6}", href)[0])
        except:
            continue
 
def getStockInfo(lst, stockURL, fpath):
    count = 0
    for stock in lst:
        url = stockURL + stock + ".html"
        html = getHTMLText(url)
        try:
            if html=="":
                continue
            infoDict = {}
            soup = BeautifulSoup(html, 'html.parser')
            stockInfo = soup.find('div',attrs={'class':'stock-bets'})
 
            name = stockInfo.find_all(attrs={'class':'bets-name'})[0]
            infoDict.update({'股票名称': name.text.split()[0]})
             
            keyList = stockInfo.find_all('dt')
            valueList = stockInfo.find_all('dd')
            for i in range(len(keyList)):
                key = keyList[i].text
                val = valueList[i].text
                infoDict[key] = val
             
            with open(fpath, 'a', encoding='utf-8') as f:
                f.write( str(infoDict) + '\n' )
                count = count + 1
                print("\r当前进度: {:.2f}%".format(count*100/len(lst)),end="")
        except:
            count = count + 1
            print("\r当前进度: {:.2f}%".format(count*100/len(lst)),end="")
            continue
 
def main():
    stock_list_url = 'https://quote.eastmoney.com/stocklist.html'
    stock_info_url = 'https://gupiao.baidu.com/stock/'
    output_file = 'D:/BaiduStockInfo.txt'
    slist=[]
    getStockList(slist, stock_list_url)
    getStockInfo(slist, stock_info_url, output_file)
 
main()
</pre>
</div>
<h2>5 网络爬虫之框架-Scrapy</h2><h3>5.1 Scrapy框架介绍</h3><p>Scrapy是一个快速功能强大的网络爬虫框架<br>安装：pip install scrapy<p>Microsoft Visual C++ 14.0 is required…报错，<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a> 网站下载对应版本<p>然后安装pip install d:\Twisted-18.7.0-cp36-cp36m-win32.whl<p>之后再安装scrapy<p>小测：scrapy ‐h<br>Scrapy不是一个函数功能库，而是一个爬虫框架<br>爬虫框架是实现爬虫功能的一个软件结构和功能组件集合<br>爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫<img width="1118" height="578" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)29.png" border="0"><p>1 Engine从Spider处获得爬取请求(Request)<br>2 Engine将爬取请求转发给Scheduler，用于调度<br>3 Engine从Scheduler处获得下一个要爬取的请求<br>4 Engine将爬取请求通过中间件发送给Downloader<br>5 爬取网页后，Downloader形成响应（Response）通过中间件发给Engine<br>6 Engine将收到的响应通过中间件发送给Spider处理<br>7 Spider处理响应后产生爬取项（scraped Item）和新的爬取请求（Requests）给Engine<br>8 Engine将爬取项发送给Item Pipeline（框架出口）<br>9 Engine将爬取请求发送给Scheduler<br>Engine控制各模块数据流，不间断从Scheduler处获得爬取请求，直至请求为空<br>框架入口：Spider的初始爬取请求<br>框架出口：Item Pipeline<p>Engine<br>(1) 控制所有模块之间的数据流<br>(2) 根据条件触发事件<br>不需要用户修改<p>Downloader<br>根据请求下载网页<br>不需要用户修改<p>Scheduler<br>对所有爬取请求进行调度管理<br>不需要用户修改<p>Downloader Middleware<br>目的：实施Engine、Scheduler和Downloader之间进行用户可配置的控制<br>功能：修改、丢弃、新增请求或响应<br>用户可以编写配置代码<p>Spider<br>(1) 解析Downloader返回的响应（Response）<br>(2) 产生爬取项（scraped item）<br>(3) 产生额外的爬取请求（Request）<br>需要用户编写配置代码<p>Item Pipelines<br>(1) 以流水线方式处理Spider产生的爬取项<br>(2) 由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型<br>(3) 可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库<br>需要用户编写配置代码<p>Spider Middleware<br>目的：对请求和爬取项的再处理<br>功能：修改、丢弃、新增请求或爬取项<br>用户可以编写配置代码<p>requests vs. Scrapy<p>相同点：<br>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线<br>两者可用性都好，文档丰富，入门简单<br>两者都没有处理js、提交表单、应对验证码等功能（可扩展）<p>requests 库：<br>页面级爬虫<br>功能库<br>并发性考虑不足，性能较差<br>重点在于页面下载<br>定制灵活<br>上手十分简单<p>Scrapy框架：<br>网站级爬虫<br>框架<br>并发性好，性能较高<br>重点在于爬虫结构<br>一般定制灵活，深度定制困难<br>入门稍难requests<p>Scrapy是为持续运行设计的专业爬虫框架，提供操作的Scrapy命令行：scrapy &lt;command&gt; [options] [args]<p>常用命令：<br>startproject 创建一个新工程scrapy startproject &lt;name&gt; [dir]<br>genspider 创建一个爬虫scrapy genspider [options] &lt;name&gt; &lt;domain&gt;<br>settings 获得爬虫配置信息scrapy settings [options]<br>crawl 运行一个爬虫scrapy crawl &lt;spider&gt;<br>list 列出工程中所有爬虫scrapy list<br>shell 启动URL调试命令行scrapy shell [url]<h3>5.2 Scrapy爬虫基本使用</h3><p>demo.py<p><br><div class="cnblogs_Highlighter">
<pre><code>import scrapy
class DemoSpider(scrapy.Spider):
    name = "demo"
    #allowed_domains = ["python123.io"]
    start_urls = ['https://python123.io/ws/demo.html']
    def parse(self, response):
        fname = response.url.split('/')[-1]
        with open(fname, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s.' % name)
</pre>
</div>
<p>
步骤1：建立一个Scrapy爬虫工程 scrapy startproject python123demo<img width="725" height="581" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)30.png" border="0"></p><p>步骤2：在工程中产生一个Scrapy爬虫<br>进入demo目录中运行 scrapy genspider dem python123.io<p>该命令作用：<br>(1) 生成一个名称为demo的spider<br>(2) 在spiders目录下增加代码文件demo.py<br>该命令仅用于生成demo.py，该文件也可以手工生成<p>步骤3：配置产生的spider爬虫<br>配置：（1）初始URL地址（2）获取页面后的解析方式<p>步骤4：运行爬虫，获取网页<br>在命令行下，执行如下命令：scrapy crawl demo<br>demo爬虫被执行，捕获页面存储在demo.html<img width="705" height="932" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)31.png" border="0"><p>yield 生成器<p>包含yield语句的函数是一个生成器<br>生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值<br>生成器是一个不断产生值的函数<br>生成器每调用一次在yield位置产生一个值，直到函数执行结束<img width="656" height="1249" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)32.png" border="0"><img width="715" height="330" title="image" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" alt="image" src="./images/【学习笔记】PYTHON网络爬虫与信息提取(北理工 嵩天)33.png" border="0"><p>Scrapy爬虫的数据类型：<p>Request类&nbsp; class scrapy.http.Request() Request对象表示一个HTTP请求 由Spider生成，由Downloader执行<br>.url Request对应的请求URL地址<br>.method 对应的请求方法，’GET’ ‘POST’等<br>.headers 字典类型风格的请求头<br>.body 请求内容主体，字符串类型<br>.meta 用户添加的扩展信息，在Scrapy内部模块间传递信息使用<br>.copy() 复制该请求<p>Response类 class scrapy.http.Response()&nbsp; Response对象表示一个HTTP响应 由Downloader生成，由Spider处理<br>.url Response对应的URL地址<br>.status HTTP状态码，默认是200<br>.headers Response对应的头部信息<br>.body Response对应的内容信息，字符串类型<br>.flags 一组标记<br>.request 产生Response类型对应的Request对象<br>.copy() 复制该响应<p>Item类class scrapy.item.Item()<p>Item对象表示一个从HTML页面中提取的信息内容<br>由Spider生成，由Item Pipeline处理<br>Item类似字典类型，可以按照字典类型操作<p>Scrapy爬虫支持多种HTML信息提取方法：<br>• Beautiful Soup<br>• lxml<br>• re<br>• XPath Selector<br>• CSS Selector<h3>5.3 股票数据爬虫scrapy实例</h3><p>scrapy startproject BaiduStocks<p>scrapy genspider stocks baidu.com<p>stocks.py文件源代码</p><div class="cnblogs_Highlighter">
<pre><code># -*- coding: utf-8 -*-
import scrapy
import re
 
 
class StocksSpider(scrapy.Spider):
    name = "stocks"
    start_urls = ['https://quote.eastmoney.com/stocklist.html']
 
    def parse(self, response):
        for href in response.css('a::attr(href)').extract():
            try:
                stock = re.findall(r"[s][hz]\d{6}", href)[0]
                url = 'https://gupiao.baidu.com/stock/' + stock + '.html'
                yield scrapy.Request(url, callback=self.parse_stock)
            except:
                continue
 
    def parse_stock(self, response):
        infoDict = {}
        stockInfo = response.css('.stock-bets')
        name = stockInfo.css('.bets-name').extract()[0]
        keyList = stockInfo.css('dt').extract()
        valueList = stockInfo.css('dd').extract()
        for i in range(len(keyList)):
            key = re.findall(r'&gt;.*', keyList[i])[0][1:-5]
            try:
                val = re.findall(r'\d+\.?.*', valueList[i])[0][0:-5]
            except:
                val = '--'
            infoDict[key]=val
 
        infoDict.update(
            {'股票名称': re.findall('\s.*\(',name)[0].split()[0] + \
             re.findall('\&gt;.*\&lt;', name)[0][1:-1]})
        yield infoDict




</pre>
</div>
<p>
下面是pipelines.py文件源代码：
</p><div class="cnblogs_Highlighter">
<pre><code>
# -*- coding: utf-8 -*-
 
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
 
 
class BaidustocksPipeline(object):
    def process_item(self, item, spider):
        return item
 
class BaidustocksInfoPipeline(object):
    def open_spider(self, spider):
        self.f = open('BaiduStockInfo.txt', 'w')
 
    def close_spider(self, spider):
        self.f.close()
 
    def process_item(self, item, spider):
        try:
            line = str(dict(item)) + '\n'
            self.f.write(line)
        except:
            pass
        return item
</pre>
</div>
<p>
下面是settings.py文件中被修改的区域：
</p><div class="cnblogs_Highlighter">
<pre><code># Configure item pipelines
# See https://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'BaiduStocks.pipelines.BaidustocksInfoPipeline': 300,
}
</pre>
</div>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>