<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）</center></div><div class='banquan'>原文出处:本文由博客园博主极客挖掘机提供。<br/>
原文连接:https://www.cnblogs.com/babycomeon/p/12057994.html</div><br>
    <h1 id="小白学-python-爬虫21解析库-beautiful-soup上">小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）</h1>
<p><img src="./images/小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）0.png" /></p>
<blockquote>
<p>人生苦短，我用 Python</p>
</blockquote>
<p>前文传送门：</p>
<p><a href="https://www.geekdigging.com/2019/11/13/3303836941/">小白学 Python 爬虫（1）：开篇</a></p>
<p><a href="https://www.geekdigging.com/2019/11/20/2586166930/">小白学 Python 爬虫（2）：前置准备（一）基本类库的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/21/1005563697/">小白学 Python 爬虫（3）：前置准备（二）Linux基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/22/3679472340/">小白学 Python 爬虫（4）：前置准备（三）Docker基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/24/334078215/">小白学 Python 爬虫（5）：前置准备（四）数据库基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/25/1881661601/">小白学 Python 爬虫（6）：前置准备（五）爬虫框架的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/26/1197821400/">小白学 Python 爬虫（7）：HTTP 基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/27/101847406/">小白学 Python 爬虫（8）：网页基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/28/1668465912/">小白学 Python 爬虫（9）：爬虫基础</a></p>
<p><a href="https://www.geekdigging.com/2019/12/01/2475257648/">小白学 Python 爬虫（10）：Session 和 Cookies</a></p>
<p><a href="https://www.geekdigging.com/2019/12/02/2333822325/">小白学 Python 爬虫（11）：urllib 基础使用（一）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/03/819896244/">小白学 Python 爬虫（12）：urllib 基础使用（二）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/04/2992515886/">小白学 Python 爬虫（13）：urllib 基础使用（三）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/05/104488944/">小白学 Python 爬虫（14）：urllib 基础使用（四）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/07/2788855167/">小白学 Python 爬虫（15）：urllib 基础使用（五）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/09/1691033431/">小白学 Python 爬虫（16）：urllib 实战之爬取妹子图</a></p>
<p><a href="https://www.geekdigging.com/2019/12/10/1910005577/">小白学 Python 爬虫（17）：Requests 基础使用</a></p>
<p><a href="https://www.geekdigging.com/2019/12/11/1468953802/">小白学 Python 爬虫（18）：Requests 进阶操作</a></p>
<p><a href="https://www.geekdigging.com/2019/12/12/3568648672/">小白学 Python 爬虫（19）：Xpath 基操</a></p>
<p><a href="https://www.geekdigging.com/2019/12/13/2569867940/">小白学 Python 爬虫（20）：Xpath 进阶</a></p>
<h2 id="引言">引言</h2>
<p>首先当然是各种资料地址敬上：</p>
<ul>
<li>官方网站：<a href="https://www.crummy.com/software/BeautifulSoup/" class="uri">https://www.crummy.com/software/BeautifulSoup/</a></li>
<li>官方文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" class="uri">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></li>
<li>中文文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" class="uri">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></li>
</ul>
<p>先看下官方对自己的介绍：</p>
<blockquote>
<p>Beautiful Soup 提供一些简单的、 Python 式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</p>
<p>Beautiful Soup 自动将输入文档转换为 Unicode 编码，输出文档转换为 UTF-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。</p>
<p>Beautiful Soup 已成为和 lxml 、 html6lib 一样出色的 Python 解释器，为用户灵活地提供不同的解析策略或强劲的速度。</p>
</blockquote>
<p>讲人话就是 Beautiful Soup 是一个非常好用、速度又快的 HTML 或 XML 的解析库。</p>
<p>Beautiful Soup 在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器。下表列出了主要的解析器,以及它们的优缺点（以下内容来自：<a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#" class="uri">https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#</a> ）:</p>
<table>
<thead>
<tr class="header">
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Python 标准库</td>
<td><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td>
<td>Python的内置标准库、执行速度适中、文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差</td>
</tr>
<tr class="even">
<td>lxml HTML 解析器</td>
<td><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td>
<td>速度快、文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr class="odd">
<td>lxml XML 解析器</td>
<td><code>BeautifulSoup(markup, [&quot;lxml-xml&quot;])</code> 、 <code>BeautifulSoup(markup, &quot;xml&quot;)</code></td>
<td>速度快、唯一支持XML的解析器</td>
<td>需要安装C语言库</td>
</tr>
<tr class="even">
<td>html5lib</td>
<td><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td>
<td>最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档</td>
<td>速度慢、不依赖外部扩展</td>
</tr>
</tbody>
</table>
<p>推荐使用 lxml 作为解析器，因为效率更高。在 Python2.7.3 之前的版本和 Python3 中 3.2.2 之前的版本，必须安装 lxml 或 html5lib ，因为那些 Python 版本的标准库中内置的 HTML 解析方法不够稳定。</p>
<p>提示: 如果一段 HTML 或 XML 文档格式不正确的话，那么在不同的解析器中返回的结果可能是不一样的，查看 <a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#id53">解析器之间的区别</a> 了解更多细节。</p>
<h2 id="基本操作">基本操作</h2>
<p>爬取对象还是小编的个人博客（小编看着博客的流量在暗暗心痛）。最基本的，还是先打印首页的 HTML 源码，使用的类库为 Requests + bs4。</p>
<pre><code><code>import requests
from bs4 import BeautifulSoup

response = requests.get(&#39;https://www.geekdigging.com/&#39;)
soup = BeautifulSoup(response.content, &quot;html5lib&quot;)
print(soup.prettify())</code></pre>
<p>结果就不贴了，太长，浪费大家翻页的时间。</p>
<p>首先先解释一下这里为什么选择了 <code>html5lib</code> 的解析器而不是 lxml 的解析器，因为经过小编测试 lxml 的解析器无法解析某些 HTML 标签，经过小编的测试，使用 Python 标准库或者 <code>html5lib</code> 解析器都无此问题，所以这里选择使用 Python 标准库。</p>
<p>上面这段代码主要是调用了 <code>prettify()</code> ，这个方法的主要作用是把要解析的字符串以标准的缩进格式输出。值得注意的是，这里的输出会自动更正 HTML 的格式，但是这一步并不是由 <code>prettify()</code> 这个方法来做的，而是在初始化 BeautifulSoup 时就完成了。</p>
<h2 id="节点选择">节点选择</h2>
<p>我们使用 Beautiful Soup 的目的是什么？当然是要选择我们需要的节点，并从节点中提取出我们需要的数据。</p>
<p>Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是 Python 对象,所有对象可以归纳为4种: <code>Tag</code> , <code>NavigableString</code> , <code>BeautifulSoup</code> , <code>Comment</code> 。</p>
<p>我们直接调用节点的名称就可以选择节点元素，再调用 string 属性就可以得到节点内的文本了，这种选择方式速度非常快。</p>
<pre><code><code>print(soup.title)
print(type(soup.title))
print(soup.title.string)
print(soup.a)</code></pre>
<p>结果如下：</p>
<pre><code><code>&lt;title&gt;极客挖掘机&lt;/title&gt;
&lt;class &#39;bs4.element.Tag&#39;&gt;
极客挖掘机
&lt;a class=&quot;logo&quot; href=&quot;/&quot;&gt;
&lt;img src=&quot;/favicon.jpg&quot; style=&quot;margin-right: 10px;&quot;/&gt;极客挖掘机
&lt;/a&gt;</code></pre>
<p>可以看到，我们这里直接输出的 <code>title</code> 节点，它的类型是 <code>bs4.element.Tag</code> ，并且使用 string 属性，直接得到了该节点的内容。</p>
<p>这里我们打印了 <code>a</code> 节点，可以看到，只打印出来了第一个 <code>a</code> 节点，后面的节点并未打印，说明当有多个节点时，这种方式只能获得第一个节点。</p>
<h3 id="获取名称">获取名称</h3>
<p>每个 tag 都有自己的名字，通过 <code>.name</code> 来获取：</p>
<pre><code><code>tag = soup.section
print(tag.name)</code></pre>
<p>结果如下：</p>
<pre><code><code>section</code></pre>
<h3 id="获取属性">获取属性</h3>
<p>一个 tag 可能有很多个属性， tag 的属性的操作方法与字典相同：</p>
<pre><code><code>print(tag[&#39;class&#39;])</code></pre>
<p>结果如下：</p>
<pre><code><code>[&#39;content-wrap&#39;]</code></pre>
<p>也可以直接”点”取属性, 比如: <code>.attrs</code> ：</p>
<pre><code><code>print(tag.attrs)</code></pre>
<p>结果如下：</p>
<pre><code><code>{&#39;class&#39;: [&#39;content-wrap&#39;]}</code></pre>
<h3 id="获取内容">获取内容</h3>
<p>可以利用 <code>string</code> 属性获取节点元素包含的文本内容，比如要获取 <code>title</code> 标签的内容：</p>
<pre><code><code>print(soup.title.string)</code></pre>
<p>结果如下：</p>
<pre><code><code>极客挖掘机</code></pre>
<h3 id="嵌套选择">嵌套选择</h3>
<p>在上面的示例中，我们的信息都是从通过 tag 的属性获得的，当然 tag 是可以继续嵌套的选择下去，比如我们刚才获取了第一个 <code>a</code> 标签，我们可以继续获取其中的 <code>img</code> 标签：</p>
<pre><code><code>print(soup.a.img)
print(type(soup.a.img))
print(soup.a.img.attrs)</code></pre>
<p>结果如下：</p>
<pre><code><code>&lt;img src=&quot;/favicon.jpg&quot; style=&quot;margin-right: 10px;&quot;/&gt;
&lt;class &#39;bs4.element.Tag&#39;&gt;
{&#39;src&#39;: &#39;/favicon.jpg&#39;, &#39;style&#39;: &#39;margin-right: 10px;&#39;}</code></pre>
<p>可以看到我们在 <code>a</code> 标签上继续选择的 <code>img</code> 标签，它的类型依然是 <code>bs4.element.Tag</code> ，并且我们成功的获取了 <code>img</code> 标签的属性值，也就是说，我们在 Tag 类型的基础上再次选择得到的依然还是 Tag 类型，所以这样就可以做嵌套选择了。</p>
<h3 id="关联选择">关联选择</h3>
<p>在选择节点的时候，我们很少可以一步到位，直接选到所需要的节点，这就需要我们先选中其中的某一个节点，再已它为基准，再选择它的子节点、父节点或者兄弟节点。</p>
<h4 id="子节点">子节点</h4>
<p>获取子节点，我们可以选择使用 contents 属性，示例如下：</p>
<pre><code><code>print(soup.article.contents)</code></pre>
<p>结果太长了，小编就不贴了，这里输出了第一个 <code>article</code> 的所有节点，并且返回结果是列表形式。 <code>article</code> 节点里既包含文本，又包含节点，最后会将它们以列表形式统一返回。</p>
<p>这里需要注意的是，列表中的每个元素都是 <code>article</code> 的直接子节点，并没有将再下一级的元素列出来。而使用 <code>children</code> 也可以得到相同的效果。</p>
<pre><code><code>for child in enumerate(soup.article.children):
    print(child)</code></pre>
<p>结果得到的还是相同的的 HTML 文本，这里调用了 <code>children</code> 属性来选择，返回结果是生成器类型。</p>
<p>想要得到所有的孙子节点的话，可以使用 <code>descendants</code> ：</p>
<pre><code><code>for i, child in enumerate(soup.article.descendants):
    print(i, child)</code></pre>
<h4 id="父节点">父节点</h4>
<p>获取父节点可以使用 <code>parent</code> 属性，示例如下：</p>
<pre><code><code>print(soup.title.parent)</code></pre>
<p>结果有些长，就不贴了，各位同学可以自行尝试一下。</p>
<h4 id="兄弟节点">兄弟节点</h4>
<p>想要获取兄弟节点可以使用属性 <code>next_sibling</code> 和 <code>previous_sibling</code> 。</p>
<pre><code><code>print(&#39;next_sibling：&#39;, soup.title.next_sibling)
print(&#39;previous_sibling：&#39;, soup.title.previous_sibling)
print(&#39;next_siblings：&#39;, soup.title.next_siblings)
print(&#39;previous_siblings：&#39;, soup.title.previous_siblings)</code></pre>
<p>结果如下：</p>
<pre><code><code>next_sibling： 


    
previous_sibling： 



next_siblings： &lt;generator object PageElement.next_siblings at 0x00000183342C5D48&gt;
previous_siblings： &lt;generator object PageElement.previous_siblings at 0x00000183342C5D48&gt;</code></pre>
<p>可以看到， next_sibling 和 previous_sibling 分别获取节点的下一个和上一个兄弟元素，在这里并没有获取到值，而是获取了很多的空行，这个是在初始化 BeautifulSoup 的时候初始化出来的，而 next_siblings 和 previous_siblings 则分别返回所有前面和后面的兄弟节点的生成器。</p>
<h2 id="示例代码">示例代码</h2>
<p>本系列的所有代码小编都会放在代码管理仓库 Github 和 Gitee 上，方便大家取用。</p>
<p><a href="https://github.com/meteor1993/python-learning/tree/master/python-spider/bs4-demo" title="示例代码-Github">示例代码-Github</a></p>
<p><a href="https://gitee.com/inwsy/python-learning/tree/master/python-spider/bs4-demo" title="示例代码-Gitee">示例代码-Gitee</a></p>
<h2 id="参考">参考</h2>
<p><a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#" class="uri">https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#</a></p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>