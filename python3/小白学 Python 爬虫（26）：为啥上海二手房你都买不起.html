<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修小白学 Python 爬虫（26）：为啥上海二手房你都买不起' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>小白学 Python 爬虫（26）：为啥上海二手房你都买不起</center></div><div class='banquan'>原文出处:本文由博客园博主极客挖掘机提供。<br/>
原文连接:https://www.cnblogs.com/babycomeon/p/12094742.html</div><br>
    <p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起0.png" /></p>
<blockquote>
<p>人生苦短，我用 Python</p>
</blockquote>
<p>前文传送门：</p>
<p><a href="https://www.geekdigging.com/2019/11/13/3303836941/">小白学 Python 爬虫（1）：开篇</a></p>
<p><a href="https://www.geekdigging.com/2019/11/20/2586166930/">小白学 Python 爬虫（2）：前置准备（一）基本类库的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/21/1005563697/">小白学 Python 爬虫（3）：前置准备（二）Linux基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/22/3679472340/">小白学 Python 爬虫（4）：前置准备（三）Docker基础入门</a></p>
<p><a href="https://www.geekdigging.com/2019/11/24/334078215/">小白学 Python 爬虫（5）：前置准备（四）数据库基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/25/1881661601/">小白学 Python 爬虫（6）：前置准备（五）爬虫框架的安装</a></p>
<p><a href="https://www.geekdigging.com/2019/11/26/1197821400/">小白学 Python 爬虫（7）：HTTP 基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/27/101847406/">小白学 Python 爬虫（8）：网页基础</a></p>
<p><a href="https://www.geekdigging.com/2019/11/28/1668465912/">小白学 Python 爬虫（9）：爬虫基础</a></p>
<p><a href="https://www.geekdigging.com/2019/12/01/2475257648/">小白学 Python 爬虫（10）：Session 和 Cookies</a></p>
<p><a href="https://www.geekdigging.com/2019/12/02/2333822325/">小白学 Python 爬虫（11）：urllib 基础使用（一）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/03/819896244/">小白学 Python 爬虫（12）：urllib 基础使用（二）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/04/2992515886/">小白学 Python 爬虫（13）：urllib 基础使用（三）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/05/104488944/">小白学 Python 爬虫（14）：urllib 基础使用（四）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/07/2788855167/">小白学 Python 爬虫（15）：urllib 基础使用（五）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/09/1691033431/">小白学 Python 爬虫（16）：urllib 实战之爬取妹子图</a></p>
<p><a href="https://www.geekdigging.com/2019/12/10/1910005577/">小白学 Python 爬虫（17）：Requests 基础使用</a></p>
<p><a href="https://www.geekdigging.com/2019/12/11/1468953802/">小白学 Python 爬虫（18）：Requests 进阶操作</a></p>
<p><a href="https://www.geekdigging.com/2019/12/12/3568648672/">小白学 Python 爬虫（19）：Xpath 基操</a></p>
<p><a href="https://www.geekdigging.com/2019/12/13/2569867940/">小白学 Python 爬虫（20）：Xpath 进阶</a></p>
<p><a href="https://www.geekdigging.com/2019/12/15/2789385418/">小白学 Python 爬虫（21）：解析库 Beautiful Soup（上）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/16/876770087/">小白学 Python 爬虫（22）：解析库 Beautiful Soup（下）</a></p>
<p><a href="https://www.geekdigging.com/2019/12/17/876770088/">小白学 Python 爬虫（23）：解析库 pyquery 入门</a></p>
<p><a href="https://www.geekdigging.com/2019/12/18/1275791678/">小白学 Python 爬虫（24）：2019 豆瓣电影排行</a></p>
<p><a href="https://www.geekdigging.com/2019/12/19/1066903974/">小白学 Python 爬虫（25）：爬取股票信息</a></p>
<h2 id="引言">引言</h2>
<p>看到题目肯定有同学会问，为啥不包含新房，emmmmmmmmmmm</p>
<p>说出来都是血泪史啊。。。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起1.png" /></p>
<p>小编已经哭晕在厕所，那位同学赶紧醒醒，太阳还没下山呢。</p>
<p>别看不起二手房，说的好像大家都买得起一样。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起2.png" /></p>
<h2 id="分析">分析</h2>
<p>淡不多扯，先进入正题，目标页面的链接小编已经找好了：<a href="https://sh.lianjia.com/ershoufang/pg1/" class="uri">https://sh.lianjia.com/ershoufang/pg1/</a> 。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起3.png" /></p>
<p>房源数量还是蛮多的么，今年正题房产行业不景气，<strong>据说</strong> 房价都不高。</p>
<p>小编其实是有目的的，毕竟也来上海五年多了，万一真的爬出来的数据看到有合适，对吧，顺便也能帮大家探个路。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起4.png" /></p>
<p>首先还是分析页面的链接信息，其实已经很明显了，在链接最后一栏有一个 <code>pg1</code> ，小编猜应该是 <code>page1</code> 的意思，不信换成 <code>pg2</code> 试试看，很显然的么。</p>
<p>随便打开一个房屋页面进到内层页面，看下数据：</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起5.png" /></p>
<p>数据还是很全面的嘛，那详细数据就从这里取了。</p>
<p>顺便再看下详情页的链接：<a href="https://sh.lianjia.com/ershoufang/107102012982.html" class="uri">https://sh.lianjia.com/ershoufang/107102012982.html</a> 。</p>
<p>这个编号从哪里来？</p>
<p>小编敢保证在外层列表页的 DOM 结构里肯定能找到。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起6.png" /></p>
<p>这就叫老司机的直觉，秀不秀就完了。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起7.png" /></p>
<h2 id="撸代码">撸代码</h2>
<p>思想还是老思想，先将外层列表页的数据构建一个列表，然后通过循环那个列表爬取详情页，将获取到的数据写入 Mysql 中。</p>
<p>本篇所使用到的请求库和解析库还是 Requests 和 pyquery 。</p>
<p>别问为啥，问就是小编喜欢。</p>
<p>因为简单。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起8.png" /></p>
<p>还是先定义一个爬取外层房源列表的方法：</p>
<pre><code><code>def get_outer_list(maxNum):
    list = []
    for i in range(1, maxNum + 1):
        url = &#39;https://sh.lianjia.com/ershoufang/pg&#39; + str(i)
        print(&#39;正在爬取的链接为： %s&#39; %url)
        response = requests.get(url, headers=headers)
        print(&#39;正在获取第 %d 页房源&#39; % i)
        doc = PyQuery(response.text)
        num = 0
        for item in doc(&#39;.sellListContent li&#39;).items():
            num += 1
            list.append(item.attr(&#39;data-lj_action_housedel_id&#39;))
        print(&#39;当前页面房源共 %d 套&#39; %num)
    return list</code></pre>
<p>这里先获取房源的那个 <code>id</code> 编号列表，方便我们下一步进行连接的拼接，这里的传入参数是最大页数，只要不超过实际页数即可，目前最大页数是 100 页，这里最大也只能传入 100 。</p>
<p>房源列表获取到以后，接着就是要获取房源的详细信息，这次的信息量有点大，解析起来稍有费劲儿：</p>
<pre><code><code>def get_inner_info(list):
    for i in list:
        try:
            response = requests.get(&#39;https://sh.lianjia.com/ershoufang/&#39; + str(i) + &#39;.html&#39;, headers=headers)
            doc = PyQuery(response.text)

            # 基本属性解析
            base_li_item = doc(&#39;.base .content ul li&#39;).remove(&#39;.label&#39;).items()
            base_li_list = []
            for item in base_li_item:
                base_li_list.append(item.text())

            # 交易属性解析
            transaction_li_item = doc(&#39;.transaction .content ul li&#39;).items()
            transaction_li_list = []
            for item in transaction_li_item:
                transaction_li_list.append(item.children().not_(&#39;.label&#39;).text())

            insert_data = {
                &quot;id&quot;: i,
                &quot;danjia&quot;: doc(&#39;.unitPriceValue&#39;).remove(&#39;i&#39;).text(),
                &quot;zongjia&quot;: doc(&#39;.price .total&#39;).text() + &#39;万&#39;,
                &quot;quyu&quot;: doc(&#39;.areaName .info&#39;).text(),
                &quot;xiaoqu&quot;: doc(&#39;.communityName .info&#39;).text(),
                &quot;huxing&quot;: base_li_list[0],
                &quot;louceng&quot;: base_li_list[1],
                &quot;jianmian&quot;: base_li_list[2],
                &quot;jiegou&quot;: base_li_list[3],
                &quot;taoneimianji&quot;: base_li_list[4],
                &quot;jianzhuleixing&quot;: base_li_list[5],
                &quot;chaoxiang&quot;: base_li_list[6],
                &quot;jianzhujiegou&quot;: base_li_list[7],
                &quot;zhuangxiu&quot;: base_li_list[8],
                &quot;tihubili&quot;: base_li_list[9],
                &quot;dianti&quot;: base_li_list[10],
                &quot;chanquan&quot;: base_li_list[11],
                &quot;guapaishijian&quot;: transaction_li_list[0],
                &quot;jiaoyiquanshu&quot;: transaction_li_list[1],
                &quot;shangcijiaoyi&quot;: transaction_li_list[2],
                &quot;fangwuyongtu&quot;: transaction_li_list[3],
                &quot;fangwunianxian&quot;: transaction_li_list[4],
                &quot;chanquansuoshu&quot;: transaction_li_list[5],
                &quot;diyaxinxi&quot;: transaction_li_list[6]
            }
            cursor.execute(sql_insert, insert_data)
            conn.commit()
            print(i, &#39;：写入完成&#39;)
        except:
            print(i, &#39;：写入异常&#39;)
            continue</code></pre>
<p>两个最关键的方法已经写完了，接下来看下小编的成果：</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起9.png" /></p>
<p>这个价格看的小编血压有点高。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起10.png" /></p>
<p>果然还是我大魔都，不管几手房，价格看看就好。</p>
<h2 id="小结">小结</h2>
<p>从结果可以看出来，链家虽然是说的有 6W 多套房子，实际上我们从页面上可以爬取到的拢共也就只有 3000 套，远没有达到我们想要的所有的数据。但是小编增加筛选条件，房源总数确实也是会变动的，应该是做了强限制，最多只能展示 100 页的数据，防止数据被完全爬走。</p>
<p>套路还是很深的，只要不把数据放出来，泥萌就不要想能爬到我的数据。对于一般用户而言，能看到前面的一些数据也足够了，估计也没几个人会翻到最后几页去看数据。</p>
<p><img src="./images/小白学 Python 爬虫（26）：为啥上海二手房你都买不起11.png" /></p>
<p>本篇的代码就到这里了，如果有需要获取全部代码的，可以访问代码仓库获取。</p>
<h2 id="示例代码">示例代码</h2>
<p>本系列的所有代码小编都会放在代码管理仓库 Github 和 Gitee 上，方便大家取用。</p>
<p><a href="https://github.com/meteor1993/python-learning/tree/master/python-spider/lianjia-spider" title="示例代码-Github">示例代码-Github</a></p>
<p><a href="https://gitee.com/inwsy/python-learning/tree/master/python-spider/lianjia-spider" title="示例代码-Gitee">示例代码-Gitee</a></p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>