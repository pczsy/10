<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修python爬虫框架scrapy 豆瓣实战' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>python爬虫框架scrapy 豆瓣实战</center></div><div class='banquan'>原文出处:本文由博客园博主风，又奈何提供。<br/>
原文连接:https://www.cnblogs.com/CYHISTW/p/11491513.html</div><br>
    <h1>Scrapy</h1>
<p>官方介绍是</p>
<p>An open source and collaborative framework for extracting the data you need from websites.</p>
<p>In a fast, simple, yet extensible way.</p>
<p>意思就是</p>
<p><span>一个开源和协作框架，用于以快速，简单，可扩展的方式从网站中提取所需的数据。</span></p>
<hr />
<p>&nbsp;</p>
<h2>环境准备</h2>
<p>本文项目使用环境及工具如下</p>
<ul>
<li>python3</li>
<li>scrapy</li>
<li>mongodb</li>
</ul>
<p>python3 scrapy的安装就不再叙述</p>
<p>mongodb是用来存储数据的nosql非关系型数据库 官方下载地址<a href="https://www.mongodb.com/download-center/community?jmp=docs">https://www.mongodb.com/download-center/community?jmp=docs</a></p>
<p>mongodb图形化管理工具推荐使用nosqlmanager</p>
<hr />
<h2>&nbsp;项目创建</h2>
<p>没错，我们还是挑软柿子捏，就爬取最简单的豆瓣电影top250</p>
<p>😂这个网站几乎是每个学习爬虫的人都会去爬取的网站，这个网站特别有代表性 话不多说，项目开始</p>
<p>创建scrapy项目需要在命令行中进行</p>
<p>切换到工作目录，然后输入指令&nbsp;&nbsp;scrapy startproject douban</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战0.png" alt="" width="665" height="211" /></p>
<p>即创建成功，然后使用pycharm打开项目 首先看下目录结构</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战1.png" alt="" width="259" height="297" /></p>
<p>我们发现项目spiders中只有一个文件，放爬虫的地方怎么会只有一个__init__.py呢&nbsp;</p>
<p>别急我们还需要输入一个命令来创建基本爬虫&nbsp; 打开cmd切换到目录文件夹下的spiders目录</p>
<p>输入&nbsp;scrapy genspider douban_spider https://movie.douban.com/top250</p>
<p>如下图创建爬虫成功&nbsp;&nbsp;</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战2.png" alt="" width="799" height="69" /></p>
<p>然后我们打开项目分析目录结构</p>
<p>douban&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 项目文件夹</p>
<p>　　spiders　　　　　　　　　　　　&nbsp; &nbsp; 爬虫文件夹</p>
<p>　　　　__init__.py</p>
<p>　　　　douban_spider.py　　　　　　 爬虫文件</p>
<p>　　__init__.py</p>
<p>　　ietms.py　　　　　　　　　　　　&nbsp; 定义items数据结构的地方（即我们爬取内容的属性之类的信息）</p>
<p>　　middlewares.py　　　　　　　　　 中间件</p>
<p>　　pipelines.py　　　　　　　　　　　定义对于items的处理方法（数据清洗等）（需要在settings中开启pipelines选项）</p>
<p>　　settings.py　　　　　　　　　　　 项目的设置文件，定义全局的各种设置（比如头部代理，任务并发量，下载延迟等等）</p>
<p>scrapy.cfg&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;项目的配置文件（包含一些默认的配置信息）</p>
<p>至此我们的的项目算是创建成功了</p>
<hr />
<p>&nbsp;</p>
<h2>确定内容</h2>
<p>创建好项目之后下一步就是确定我们要爬取的内容了，然后才可以开始编写我们的items.py文件</p>
<p>首先打开目标网页进行分析&nbsp;</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战3.png" alt="" width="617" height="394" /></p>
<p>网页中有哪些东西是我们需要的呢？</p>
<ul>
<li>电影排名编号</li>
<li>电影名称</li>
<li>电影演职员以及年份分类</li>
<li>电影星级评分&nbsp;</li>
<li>评论人数</li>
<li>电影简介&nbsp;</li>
</ul>
<p>现在就可以根据内容来编写items.py文件了</p>
<p>items.py文件代码编写如下</p>
<div class="cnblogs_code">
<pre><code><span style="color: #008000;">#</span><span style="color: #008000;"> -*- coding: utf-8 -*-</span>

<span style="color: #008000;">#</span><span style="color: #008000;"> Define here the models for your scraped items</span><span style="color: #008000;">
#
#</span><span style="color: #008000;"> See documentation in:</span><span style="color: #008000;">
#</span><span style="color: #008000;"> https://docs.scrapy.org/en/latest/topics/items.html</span>

<span style="color: #0000ff;">import</span><span style="color: #000000;"> scrapy


</span><span style="color: #0000ff;">class</span><span style="color: #000000;"> DoubanItem(scrapy.Item):
    </span><span style="color: #008000;">#</span><span style="color: #008000;">示例</span>
    <span style="color: #008000;">#</span><span style="color: #008000;"> define the fields for your item here like:</span>
    <span style="color: #008000;">#</span><span style="color: #008000;"> name = scrapy.Field()</span>
<span style="color: #000000;">
    serial_number </span>= scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">排名</span>
    movie_name = scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">电影名称</span>
    introduce = scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">电影简介基本信息</span>
    star = scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">电影星级评分</span>
    evaluate = scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">电影评论人数</span>
    describe = scrapy.Field()<span style="color: #008000;">#</span><span style="color: #008000;">电影内容简介</span></pre>
</div>
<p>&nbsp;</p>
<hr />
<p>&nbsp;</p>
<h2>内容提取spider文件编写</h2>
<p>确定内容之后就是非常关键的spider爬虫文件编写了&nbsp;</p>
<p>测试阶段douban_spider.py文件编写如下：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #008000;">#</span><span style="color: #008000;"> -*- coding: utf-8 -*-</span>
<span style="color: #0000ff;">import</span><span style="color: #000000;"> scrapy


</span><span style="color: #0000ff;">class</span><span style="color: #000000;"> DoubanSpiderSpider(scrapy.Spider):
    </span><span style="color: #008000;">#</span><span style="color: #008000;">爬虫名字</span>
    name = <span style="color: #800000;">'</span><span style="color: #800000;">douban_spider</span><span style="color: #800000;">'</span>
    <span style="color: #008000;">#</span><span style="color: #008000;">允许的域名 爬取url都属于这个域名</span>
    allowed_domains = [<span style="color: #800000;">'</span><span style="color: #800000;">movie.douban.com</span><span style="color: #800000;">'</span><span style="color: #000000;">]
    </span><span style="color: #008000;">#</span><span style="color: #008000;">起始url</span>
    start_urls = [<span style="color: #800000;">'</span><span style="color: #800000;">https://movie.douban.com/top250/</span><span style="color: #800000;">'</span><span style="color: #000000;">]

    </span><span style="color: #0000ff;">def</span><span style="color: #000000;"> parse(self, response):
        </span><span style="color: #0000ff;">print</span>(response.text)<span style="color: #008000;">#</span><span style="color: #008000;">打印响应内容</span>
        <span style="color: #0000ff;">pass</span></pre>
</div>
<p>然后我们需要运行下我们的爬虫看下现在能否出什么信息</p>
<p>打开命令窗口并cd到项目目录下输入命令&nbsp; &nbsp;scrapy crawl douban_spider</p>
<p>douban_spider是爬虫的名字</p>
<p>运行如下图</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战4.png" alt="" width="583" height="330" /></p>
<p>发现里面有爬虫的信息也有返回响应的信息，但是我们可以看出来没有我们想要的电影信息，现在怎么办？</p>
<p>稍微学习过爬虫的同学都知道，爬虫是需要修改&nbsp;USER_AGENT 的，这也是最简单的反爬虫机制，所以我们同样需要去修改我们爬虫的用户代理</p>
<p>去哪找头部代理呢？简单一点的可以直接去百度搜索一个，或者呢我们用浏览器调试台把自己的用户代理复制下来</p>
<p>例如chrome浏览器按F12点击一个资源复制出来用户代理即可不再赘述</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战5.png" alt="" width="886" height="341" /></p>
<p>打开settings.py文件&nbsp; 找到USER_AGENT修改如下：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #008000;">#</span><span style="color: #008000;"> Crawl responsibly by identifying yourself (and your website) on the user-agent</span><span style="color: #008000;">
#</span><span style="color: #008000;">USER_AGENT = 'douban (+http://www.yourdomain.com)'</span>
USER_AGENT = <span style="color: #800000;">'</span><span style="color: #800000;">Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36</span><span style="color: #800000;">'</span></pre>
</div>
<p>然后我们再次打开命令窗口并cd到项目目录下输入命令&nbsp; &nbsp;scrapy crawl douban_spider</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战6.png" alt="" width="653" height="483" /></p>
<p>发现已经有了我们想要的电影信息</p>
<p>同时每次在命令行运行spider确实不方便，我们可以在项目中添加一个main.py的启动文件如下</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战7.png" alt="" width="236" height="149" /></p>
<p>main.py编写代码：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">from</span> scrapy <span style="color: #0000ff;">import</span><span style="color: #000000;"> cmdline
cmdline.execute(</span><span style="color: #800000;">'</span><span style="color: #800000;">scrapy crawl douban_spider</span><span style="color: #800000;">'</span>.split())</pre>
</div>
<p>运行，发现就得到了与命令行运行一样的效果了</p>
<p>接下来的工作就是<strong>数据处理</strong>了，提取出我们想要的信息&nbsp; 继续编写spider.py文件</p>
<p>对于数据的提取我们使用xpath定位 先来观察目标网站的元素 我们可以看到top250电影中每一页有25个电影信息 并且每个电影信息都是一个列表 li</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战8.png" alt="" width="923" height="387" /></p>
<p>xpath有好多种写法 我们可以审查元素然后编写xpath定位，或者呢直接用chrome也可以直接获取某元素的xpath路径</p>
<p>例如使用某xpath浏览器插件来找我们需要的元素 我们首先先找到每个电影的定位</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战9.png" alt="" width="917" height="349" /></p>
<p>如图我们编写&nbsp;//ol[@class='grid_view']/li/div[@class='item'] 就可以定位到当前每个电影 其实我们简单点直接写 //ol/li 也可以，但是我们最好直接精确一点 xpath语法如下</p>
<table class="dataintable">
<tbody>
<tr><th>表达式</th><th>描述</th></tr>
<tr>
<td>nodename</td>
<td>选取此节点的所有子节点。</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取。</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点。</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点。</td>
</tr>
<tr>
<td>@</td>
<td>选取属性。</td>
</tr>
</tbody>
</table>
<p>然后同理，我们就可以找到电影排名，名称，评论等等信息的xpath， 接下来在spider.py文件中引用我们的 items.py中编写的DoubanItem类 然后完成对象属性的赋值</p>
<p>spider.py文件代码：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #008000;">#</span><span style="color: #008000;"> -*- coding: utf-8 -*-</span>
<span style="color: #0000ff;">import</span><span style="color: #000000;"> scrapy
</span><span style="color: #0000ff;">from</span> douban.items <span style="color: #0000ff;">import</span><span style="color: #000000;"> DoubanItem


</span><span style="color: #0000ff;">class</span><span style="color: #000000;"> DoubanSpiderSpider(scrapy.Spider):
    </span><span style="color: #008000;">#</span><span style="color: #008000;">爬虫名字</span>
    name = <span style="color: #800000;">'</span><span style="color: #800000;">douban_spider</span><span style="color: #800000;">'</span>
    <span style="color: #008000;">#</span><span style="color: #008000;">允许的域名 爬取url都属于这个域名</span>
    allowed_domains = [<span style="color: #800000;">'</span><span style="color: #800000;">movie.douban.com</span><span style="color: #800000;">'</span><span style="color: #000000;">]
    </span><span style="color: #008000;">#</span><span style="color: #008000;">起始url</span>
    start_urls = [<span style="color: #800000;">'</span><span style="color: #800000;">https://movie.douban.com/top250/</span><span style="color: #800000;">'</span><span style="color: #000000;">]

    </span><span style="color: #008000;">#</span><span style="color: #008000;">默认解析方法</span>
    <span style="color: #0000ff;">def</span><span style="color: #000000;"> parse(self, response):
        </span><span style="color: #008000;">#</span><span style="color: #008000;"> 注意在python语句中使用xpath如 注意与原来语句单双引号的问题</span>
        movie_list=response.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">//ol[@class='grid_view']/li/div[@class='item']</span><span style="color: #800000;">"</span><span style="color: #000000;">)
        </span><span style="color: #0000ff;">for</span> movie_item <span style="color: #0000ff;">in</span><span style="color: #000000;"> movie_list:
            douban_item</span>=<span style="color: #000000;">DoubanItem()
            </span><span style="color: #008000;">#</span><span style="color: #008000;">xpath语句最后的text()是获取当前xpath的内容</span>
            <span style="color: #008000;">#</span><span style="color: #008000;"> scrapy get() getall()方法获得xpath路径的值 两种方法不同请百度</span>
            douban_item[<span style="color: #800000;">'</span><span style="color: #800000;">serial_number</span><span style="color: #800000;">'</span>] = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//em/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
            douban_item[</span><span style="color: #800000;">'</span><span style="color: #800000;">movie_name</span><span style="color: #800000;">'</span>] = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//span[@class='title']/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
            </span><span style="color: #008000;">#</span><span style="color: #008000;">介绍的内容非常不规范并且有好多行，首先使用getall()来获取，然后我们要对其进行处理</span>
            content = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//div[@class='bd']/p[1]/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).getall()
            </span><span style="color: #008000;">#</span><span style="color: #008000;">处理</span>
            contient_introduce=<span style="color: #800000;">''</span>
            <span style="color: #0000ff;">for</span> conitem <span style="color: #0000ff;">in</span><span style="color: #000000;"> content:
                content_s</span>=<span style="color: #800000;">''</span><span style="color: #000000;">.join(conitem.split())
                contient_introduce</span>=contient_introduce+content_s+<span style="color: #800000;">'</span>  <span style="color: #800000;">'</span>
            <span style="color: #008000;">#</span><span style="color: #008000;">赋值</span>
            douban_item[<span style="color: #800000;">'</span><span style="color: #800000;">introduce</span><span style="color: #800000;">'</span>] =<span style="color: #000000;"> contient_introduce
            douban_item[</span><span style="color: #800000;">'</span><span style="color: #800000;">star</span><span style="color: #800000;">'</span>] = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//span[@class='rating_num']/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
            douban_item[</span><span style="color: #800000;">'</span><span style="color: #800000;">evaluate</span><span style="color: #800000;">'</span>] = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//div[@class='star']/span[4]/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
            douban_item[</span><span style="color: #800000;">'</span><span style="color: #800000;">describe</span><span style="color: #800000;">'</span>] = movie_item.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">.//div[@class='bd']/p[2]/span/text()</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
            </span><span style="color: #008000;">#</span><span style="color: #008000;">我们需要把获取到的东西yield到douban_item中，否则我们的管道pipelines.py无法接收数据</span>
            <span style="color: #0000ff;">yield</span><span style="color: #000000;"> douban_item

        </span><span style="color: #008000;">#</span><span style="color: #008000;">我们需要自动翻页到下一页去解析数据</span>
        next_linkend=response.xpath(<span style="color: #800000;">"</span><span style="color: #800000;">//span[@class='next']/a/@href</span><span style="color: #800000;">"</span><span style="color: #000000;">).get()
        </span><span style="color: #008000;">#</span><span style="color: #008000;">判断next_linkend是否存在</span>
        <span style="color: #0000ff;">if</span><span style="color: #000000;"> next_linkend:
            next_link </span>= <span style="color: #800000;">'</span><span style="color: #800000;">https://movie.douban.com/top250/</span><span style="color: #800000;">'</span>+<span style="color: #000000;">next_linkend
            </span><span style="color: #008000;">#</span><span style="color: #008000;">同样需要yield提交到调度器中 同时添加一个回调函数(刚刚编写的数据提取函数)</span>
            <span style="color: #0000ff;">yield</span> scrapy.Request(next_link,callback=self.parse)</pre>
</div>
<hr />
<h2>数据存储</h2>
<p>我们可以使用命令直接将数据保存到 json或者csv文件如下</p>
<p>还是使用命令行&nbsp; &nbsp;cd到项目目录&nbsp;&nbsp;</p>
<p>输入命令&nbsp; scrapy crawl douban_spider -o test.json 就可以得到一个json文件</p>
<p>输入命令&nbsp;&nbsp;scrapy crawl douban_spider -o test.csv&nbsp; 就可以得到一个csv文件</p>
<p>这个csv文件可以直接用excel打开浏览，但是我们会发现存在乱码，我们可以先用notepad++打开文件改下编码方式然后保存再用excel打开即可</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战10.png" alt="" width="806" height="434" /></p>
<p><strong>存储到数据库</strong></p>
<p>接下来我们需要对pipelines.py进行编写，将数据存储到mongodb中</p>
<p>注意<span style="color: #ff0000;">我们需要在setting.py中将 ITEM_PIPELINES 的注释关掉，这样才能正常的运行pipelines.py</span></p>
<p>pipelines.py代码：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #008000;">#</span><span style="color: #008000;"> -*- coding: utf-8 -*-</span>

<span style="color: #008000;">#</span><span style="color: #008000;"> Define your item pipelines here</span><span style="color: #008000;">
#
#</span><span style="color: #008000;"> Don't forget to add your pipeline to the ITEM_PIPELINES setting</span><span style="color: #008000;">
#</span><span style="color: #008000;"> See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>

<span style="color: #0000ff;">import</span><span style="color: #000000;"> pymongo
</span><span style="color: #008000;">#</span><span style="color: #008000;">连接本地数据库   远程也可以</span>
myclient = pymongo.MongoClient(<span style="color: #800000;">"</span><span style="color: #800000;">mongodb://localhost:27017/</span><span style="color: #800000;">"</span><span style="color: #000000;">)
</span><span style="color: #008000;">#</span><span style="color: #008000;">数据库名称</span>
mydb = myclient[<span style="color: #800000;">"</span><span style="color: #800000;">douban</span><span style="color: #800000;">"</span><span style="color: #000000;">]
</span><span style="color: #008000;">#</span><span style="color: #008000;">数据表名称</span>
mysheet = mydb[<span style="color: #800000;">"</span><span style="color: #800000;">movie</span><span style="color: #800000;">"</span><span style="color: #000000;">]

</span><span style="color: #0000ff;">class</span><span style="color: #000000;"> DoubanPipeline(object):
    </span><span style="color: #008000;">#</span><span style="color: #008000;">此中的item就是刚刚yield回来的</span>
    <span style="color: #0000ff;">def</span><span style="color: #000000;"> process_item(self, item, spider):
        data</span>=<span style="color: #000000;">dict(item)
        </span><span style="color: #008000;">#</span><span style="color: #008000;">插入数据</span>
<span style="color: #000000;">        mysheet.insert(data)
        </span><span style="color: #0000ff;">return</span> item</pre>
</div>
<p>现在运行 main.py 数据就存储到数据库之中了，我们可以打开数据库查看数据</p>
<p>至此，我们的爬虫项目可以说已经完成了。</p>
<hr />
<p>&nbsp;</p>
<h2>爬虫伪装</h2>
<ul>
<li>ip代理中间件</li>
<li>user-agent中间件</li>
</ul>
<p>ip代理需要购买服务器然后可以使用先不提了&nbsp;</p>
<p>我们尝试下user-agent中间件</p>
<p>编写middlewares.py再最后新加入我们自己编写的类(文件最上端要 import random)：</p>
<div class="cnblogs_code">
<pre><code><span style="color: #0000ff;">class</span><span style="color: #000000;"> my_useragent(object):
    </span><span style="color: #0000ff;">def</span><span style="color: #000000;"> process_request(self,request,spider):
        USER_AGENT_LIST </span>=<span style="color: #000000;"> [
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20</span><span style="color: #800000;">"</span><span style="color: #000000;">,
            </span><span style="color: #800000;">"</span><span style="color: #800000;">Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52</span><span style="color: #800000;">"</span><span style="color: #000000;">,
        ]
        agent </span>=<span style="color: #000000;"> random.choice(USER_AGENT_LIST)
        request.headers[</span><span style="color: #800000;">'</span><span style="color: #800000;">User_Agent</span><span style="color: #800000;">'</span>] = agent</pre>
</div>
<p>然后去 settings.py 中开启中间件并修改为我们刚刚创建的类如下图</p>
<p><img src="./images/python爬虫框架scrapy 豆瓣实战11.png" alt="" width="604" height="96" /></p>
<p>然后再运行 main.py 就都OK了。</p>
<p>&nbsp;</p>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>