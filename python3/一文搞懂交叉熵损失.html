<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修一文搞懂交叉熵损失' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>一文搞懂交叉熵损失</center></div><div class='banquan'>原文出处:本文由博客园博主Brook_icv提供。<br/>
原文连接:https://www.cnblogs.com/wangguchangqing/p/12068084.html</div><br>
    <p>本文从信息论和最大似然估计得角度推导交叉熵作为分类损失函数的依据。</p>
<h3 id="从熵来看交叉熵损失">从熵来看交叉熵损失</h3>
<h4 id="信息量">信息量</h4>
<p>信息量来衡量一个事件的不确定性，一个事件发生的概率越大，不确定性越小，则其携带的信息量就越小。</p>
<p>设<span class="math inline">\(X\)</span>是一个离散型随机变量，其取值为集合<span class="math inline">\(X = {x_0,x_1,\dots,x_n}\)</span> ，则其概率分布函数为<span class="math inline">\(p(x) = Pr(X = x),x \in X\)</span>，则定义事件<span class="math inline">\(X = x_0\)</span> 的信息量为：<br />
<span class="math display">\[
I(x_0) = -\log(p(x_0))
\]</span><br />
当<span class="math inline">\(p(x_0) = 1\)</span>时，该事件必定发生，其信息量为0.</p>
<h4 id="熵">熵</h4>
<p>熵用来衡量一个系统的混乱程度，代表系统中信息量的总和；熵值越大，表明这个系统的不确定性就越大。</p>
<p>信息量是衡量某个事件的不确定性，而熵是衡量一个系统（所有事件）的不确定性。</p>
<p>熵的计算公式<br />
<span class="math display">\[
H(x) = -\sum_{i=1}^np(x_i)\log(p(x_i))
\]</span><br />
其中，<span class="math inline">\(p(x_i)\)</span>为事件<span class="math inline">\(X=x_i\)</span>的概率，<span class="math inline">\(-log(p(x_i))\)</span>为事件<span class="math inline">\(X=x_i\)</span>的信息量。</p>
<p>可以看出，熵是信息量的期望值，是一个随机变量（一个系统，事件所有可能性）不确定性的度量。熵值越大，随机变量的取值就越难确定，系统也就越不稳定；熵值越小，随机变量的取值也就越容易确定，系统越稳定。</p>
<h4 id="相对熵-relative-entropy">相对熵 Relative entropy</h4>
<p>相对熵也称为KL散度(Kullback-Leibler divergence)，表示同一个随机变量的两个不同分布间的距离。</p>
<p>设 <span class="math inline">\(p(x),q(x)\)</span> 分别是 离散随机变量<span class="math inline">\(X\)</span>的两个概率分布，则<span class="math inline">\(p\)</span>对<span class="math inline">\(q\)</span>的相对熵是：<br />
<span class="math display">\[
D_{KL}(p \parallel q) = \sum_i p(x_i) log(\frac{p(x_i)}{q(x_i)})
\]</span></p>
<p>相对熵具有以下性质：</p>
<ul>
<li>如果<span class="math inline">\(p(x)\)</span>和<span class="math inline">\(q(x)\)</span>的分布相同，则其相对熵等于0</li>
<li><span class="math inline">\(D_{KL}(p \parallel q) \neq D_{KL}(q \parallel p)\)</span>，也就是相对熵不具有对称性。</li>
<li>$ D_{KL}(p \parallel q) \geq 0$</li>
</ul>
<p>总的来说，相对熵是用来衡量同一个随机变量的两个不同分布之间的距离。<strong>在实际应用中，假如<span class="math inline">\(p(x)\)</span>是目标真实的分布，而<span class="math inline">\(q(x)\)</span>是预测得来的分布，为了让这两个分布尽可能的相同的，就需要最小化KL散度。</strong></p>
<h4 id="交叉熵-cross-entropy">交叉熵 Cross Entropy</h4>
<p>设 <span class="math inline">\(p(x),q(x)\)</span> 分别是 离散随机变量<span class="math inline">\(X\)</span>的两个概率分布，其中<span class="math inline">\(p(x)\)</span>是目标分布，<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的交叉熵可以看做是，使用分布<span class="math inline">\(q(x)\)</span> 表示目标分布<span class="math inline">\(p(x)\)</span>的困难程度：<br />
<span class="math display">\[
H(p,q) = \sum_ip(x_i)log\frac{1}{\log q(x_i)} = -\sum_ip(x_i)\log q(x_i)
\]</span><br />
将熵、相对熵以及交叉熵的公式放到一起，<br />
<span class="math display">\[
\begin{align}
H(p) &amp;= -\sum_{i}p(x_i) \log p(x_i) \\
D_{KL}(p \parallel q) &amp;= \sum_{i}p(x_i)\log \frac{p(x_i)}{q(x_i)} = \sum_i (p(x_i)\log p(x_i) - p(x_i) \log q(x_i)) \\
H(p,q) &amp;=  -\sum_ip(x_i)\log q(x_i)
\end{align}
\]</span><br />
通过上面三个公式就可以得到<br />
<span class="math display">\[
D_{KL}(p,q) = H(p,q)- H(p)
\]</span><br />
在机器学习中，目标的分布<span class="math inline">\(p(x)\)</span> 通常是训练数据的分布是固定，即是<span class="math inline">\(H(p)\)</span> 是一个常量。这样两个分布的交叉熵<span class="math inline">\(H(p,q)\)</span> 也就等价于最小化这两个分布的相对熵<span class="math inline">\(D_{KL}(p \parallel q)\)</span>。</p>
<p>设<span class="math inline">\(p(x)\)</span> 是目标分布（训练数据的分布），我们的目标的就让训练得到的分布<span class="math inline">\(q(x)\)</span>尽可能的接近<span class="math inline">\(p(x)\)</span>，这时候就可以最小化<span class="math inline">\(D_{KL}(p \parallel q)\)</span>，等价于最小化交叉熵<span class="math inline">\(H(p,q)\)</span> 。</p>
<h3 id="从最大似然看交叉熵">从最大似然看交叉熵</h3>
<p>设有一组训练样本<span class="math inline">\(X= \{x_1,x_2,\cdots,x_m\}\)</span> ,该样本的分布为<span class="math inline">\(p(x)\)</span> 。假设使用<span class="math inline">\(\theta\)</span> 参数化模型得到<span class="math inline">\(q(x;\theta)\)</span> ，现用这个模型来估计<span class="math inline">\(X\)</span> 的概率分布，得到似然函数<br />
<span class="math display">\[
L(\theta) = q(X; \theta) = \prod_i^mq(x_i;\theta)
\]</span><br />
最大似然估计就是求得<span class="math inline">\(\theta\)</span> 使得<span class="math inline">\(L(\theta)\)</span> 的值最大，也就是<br />
<span class="math display">\[
\theta_{ML} = arg \max_{\theta} \prod_i^mq(x_i;\theta)
\]</span><br />
对上式的两边同时取<span class="math inline">\(\log\)</span> ，等价优化<span class="math inline">\(\log\)</span> 的最大似然估计即<code>log-likelyhood</code> ，最大对数似然估计<br />
<span class="math display">\[
\theta_{ML} = arg \max_\theta \sum_i^m \log q(x_i;\theta)
\]</span><br />
对上式的右边进行缩放并不会改变<span class="math inline">\(arg \max\)</span> 的解，上式的右边除以样本的个数<span class="math inline">\(m\)</span><br />
<span class="math display">\[
\theta_{ML} = arg \max_\theta \frac{1}{m}\sum_i^m\log q(x_i;\theta)
\]</span></p>
<h4 id="和相对熵等价">和相对熵等价</h4>
<p>上式的最大化<span class="math inline">\(\theta_{ML}\)</span> 是和没有训练样本没有关联的，就需要某种变换使其可以用训练的样本分布来表示，因为训练样本的分布可以看作是已知的，也是对最大化似然的一个约束条件。</p>
<p>注意上式的<br />
<span class="math display">\[
\frac{1}{m}\sum_i^m\log q(x_i;\theta)
\]</span><br />
相当于<strong>求随机变量<span class="math inline">\(X\)</span> 的函数<span class="math inline">\(\log (X;\theta)\)</span> 的均值</strong> ，根据大数定理，<strong>随着样本容量的增加，样本的算术平均值将趋近于随机变量的期望。</strong> 也就是说<br />
<span class="math display">\[
\frac{1}{m}\sum_i^m \log q(x_i;\theta) \rightarrow E_{x\sim P}(\log q(x;\theta))
\]</span><br />
其中<span class="math inline">\(E_{X\sim P}\)</span> 表示符合样本分布<span class="math inline">\(P\)</span> 的期望，这样就将最大似然估计使用真实样本的期望来表示<br />
<span class="math display">\[
\begin{aligned}
\theta_{ML} &amp;= arg \max_{\theta} E_{x\sim P}({\log q(x;\theta)}) \\
&amp;= arg \min_{\theta} E_{x \sim P}(- \log q(x;\theta))
\end{aligned}
\]</span><br />
对右边取负号，将最大化变成最小化运算。</p>
<blockquote>
<p>上述的推导过程，可以参考 《Deep Learning》 的第五章。 但是，在书中变为期望的只有一句话，将式子的右边除以样本数量<span class="math inline">\(m\)</span> 进行缩放，从而可以将其变为<span class="math inline">\(E_{x \sim p}\log q(x;\theta)\)</span>，没有细节过程，也可能是作者默认上面的变换对读者是一直。 确实是理解不了，查了很多文章，都是对这个变换的细节含糊其辞。一个周，对这个点一直耿耿于怀，就看了些关于概率论的科普书籍，其中共有介绍大数定理的：<strong>当样本容量趋于无穷时，样本的均值趋于其期望</strong>。</p>
<p>针对上面公式，除以<span class="math inline">\(m\)</span>后，<span class="math inline">\(\frac{1}{m}\sum_i^m\log q(x_i;\theta)\)</span> ，确实是关于随机变量函数<span class="math inline">\(\log q(x)\)</span> 的算术平均值，而<span class="math inline">\(x\)</span> 是训练样本其分布是已知的<span class="math inline">\(p(x)\)</span> ，这样就得到了<span class="math inline">\(E_{x \sim p}(\log q(x))\)</span> 。</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
D_{KL}(p \parallel q) &amp;= \sum_i p(x_i) log(\frac{p(x_i)}{q(x_i)})\\
&amp;= E_{x\sim p}(\log \frac{p(x)}{q(x)}) \\
&amp;= E_{x \sim p}(\log p(x) - \log q(x)) \\
&amp;= E_{x \sim p}(\log p(x)) - E_{x \sim p} (\log q(x))
\end{aligned}
\]</span></p>
<p>由于<span class="math inline">\(E_{x \sim p} (\log p(x))\)</span> 是训练样本的期望，是个固定的常数，在求最小值时可以忽略，所以最小化<span class="math inline">\(D_{KL}(p \parallel q)\)</span> 就变成了最小化<span class="math inline">\(-E_{x\sim p}(\log q(x))\)</span> ，这和最大似然估计是等价的。</p>
<h4 id="和交叉熵等价">和交叉熵等价</h4>
<p>最大似然估计、相对熵、交叉熵的公式如下<br />
<span class="math display">\[
\begin{aligned}\theta_{ML} &amp;= -arg \min_\theta E_{x\sim p}\log q(x;\theta) \\D_{KL} &amp;= E_{x \sim p}\log p(x) - E_{x \sim p} \log q(x) \\H(p,q) &amp;= -\sum_i^m p(x_i) \log q(x_i) = -E_{x \sim p} \log q(x)\end{aligned}\begin{aligned}\theta_{ML} &amp;= arg \min_\theta E_{x\sim p}\log q(x;\theta) \\D_{KL} &amp;= E_{x \sim p}\log p(x) - E_{x \sim p} \log q(x) \\H(p,q) &amp;= -\sum_i^m p(x_i) \log q(x_i) = -E_{x \sim p} \log q(x)\end{aligned}
\]</span><br />
从上面可以看出，最小化交叉熵，也就是最小化<span class="math inline">\(D_{KL}\)</span> ，从而预测的分布<span class="math inline">\(q(x)\)</span> 和训练样本的真实分布<span class="math inline">\(p(x)\)</span> 最接近。而最小化<span class="math inline">\(D_{KL}\)</span> 和最大似然估计是等价的。</p>
<h3 id="多分类交叉熵">多分类交叉熵</h3>
<p>多分类任务中输出的是目标属于<strong>每个类别的概率，所有类别概率的和为1，其中概率最大的类别就是目标所属的分类。</strong> 而<code>softmax</code> 函数能将一个向量的每个分量映射到<span class="math inline">\([0,1]\)</span> 区间，并且对整个向量的输出做了归一化，保证所有分量输出的和为1，正好满足多分类任务的输出要求。所以，在多分类中，在最后就需要将提取的到特征经过<code>softmax</code>函数的，输出为每个类别的概率，然后再使用<strong>交叉熵</strong> 作为损失函数。</p>
<p><code>softmax</code>函数定义如下：<br />
<span class="math display">\[
S_i = \frac{e^{z_i}}{\sum^n_{i=1}e^{z_i}}
\]</span><br />
其中，输入的向量为<span class="math inline">\(z_i(i = 1,2,\dots,n)\)</span> 。</p>
<p>更直观的参见下图</p>
<p><img src="./images/一文搞懂交叉熵损失0.png" /></p>
<p>通过前面的特征提取到的特征向量为<span class="math inline">\((z_1,z_2,\dots,z_k)\)</span> ，将向量输入到<code>softmax</code>函数中，即可得到目标属于每个类别的概率，概率最大的就是预测得到的目标的类别。</p>
<h4 id="cross-entropy-loss">Cross Entropy Loss</h4>
<p>使用<code>softmax</code>函数可以将特征向量映射为所属类别的概率，可以看作是预测类别的概率分布<span class="math inline">\(q(c_i)\)</span> ，有<br />
<span class="math display">\[
q(c_i) = \frac{e^{z_i}}{\sum^n_{i=1}e^{z_i}}
\]</span></p>
<p>其中<span class="math inline">\(c_i\)</span> 为某个类别。</p>
<p>设训练数据中类别的概率分布为<span class="math inline">\(p(c_i)\)</span> ，那么目标分布<span class="math inline">\(p(c_i)\)</span> 和预测分布<span class="math inline">\(q(c_i)\)</span>的交叉熵为<br />
<span class="math display">\[
H(p,q) =-\sum_ip(c_i)\log q(c_i)
\]</span><br />
每个训练样本所属的类别是已知的，并且每个样本只会属于一个类别（概率为1），属于其他类别概率为0。具体的，可以假设有个三分类任务，三个类分别是：猫，猪，狗。现有一个训练样本类别为猫，则有：<br />
<span class="math display">\[
\begin{align}
p(cat) &amp; = 1 \\
p(pig) &amp;= 0 \\
p(dog) &amp; = 0
\end{align}
\]</span></p>
<p>通过预测得到的三个类别的概率分别为：<span class="math inline">\(q(cat) = 0.6,q(pig) = 0.2,q(dog) = 0.2\)</span> ，计算<span class="math inline">\(p\)</span> 和<span class="math inline">\(q\)</span> 的交叉熵为：<br />
<span class="math display">\[
\begin{aligned}
H(p,q) &amp;= -(p(cat) \log q(cat) + p(pig) + \log q(pig) + \log q(dog)) \\
&amp;= - (1 \cdot \log 0.6 + 0 \cdot \log 0.2 +0 \cdot \log 0.2) \\
&amp;= - \log 0.6 \\
&amp;= - \log q(cat)
\end{aligned}
\]</span><br />
利用这种特性，可以将样本的类别进行重新编码，就可以简化交叉熵的计算，这种编码方式就是<strong>one-hot</strong> 编码。以上面例子为例，<br />
<span class="math display">\[
\begin{aligned}
\text{cat} &amp;= (1 0 0) \\
\text{pig} &amp;= (010) \\
\text{dog} &amp;= (001)
\end{aligned}
\]</span><br />
通过这种编码方式，在计算交叉熵时，只需要计算和训练样本对应类别预测概率的值，其他的项都是<span class="math inline">\(0 \cdot \log q(c_i) = 0\)</span> 。</p>
<p>具体的，交叉熵计算公式变成如下：<br />
<span class="math display">\[
\text{Cross_Entropy}(p,q) = - \log q(c_i)
\]</span><br />
其中<span class="math inline">\(c_i\)</span> 为训练样本对应的类别，上式也被称为<strong>负对数似然（negative log-likelihood,nll）</strong>。</p>
<h4 id="pytorch中的cross-entropy">PyTorch中的Cross Entropy</h4>
<p>PyTorch中实现交叉熵损失的有三个函数<code>torch.nn.CrossEntropyLoss</code>，<code>torch.nn.LogSoftmax</code>以及<code>torch.nn.NLLLoss</code>。</p>
<ul>
<li><code>torch.nn.functional.log_softmax</code> 比较简单，输入为<span class="math inline">\(n\)</span>维向量，指定要计算的维度<code>dim</code>，输出为<span class="math inline">\(log(Softmax(x))\)</span>。其计算公式如下：</li>
</ul>
<p><span class="math display">\[
\text{LogSoftmax}(x_i) = \log (\frac{\exp(x_i)}{\sum_j \exp(x_j)})
\]</span></p>
<p>没有额外的处理，就是对输入的<span class="math inline">\(n\)</span>维向量的每个元素进行上述运算。</p>
<ul>
<li><code>torch.nn.functional.nll_loss</code> 负对数似然损失（Negative Log Likelihood Loss)，用于多分类，其输入的通常是<code>torch.nn.functional.log_softmax</code>的输出值。其函数如下</li>
</ul>
<pre><code><code>torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre>
<p><code>input</code> 也就是<code>log_softmax</code>的输出值，各个类别的对数概率。<code>target</code> 目标正确类别,<code>weight</code> 针对类别不平衡问题，可以为类别设置不同的权值；<code>ignore_index</code> 要忽略的类别，不参与loss的计算；比较重要的是<code>reduction</code> 的值，有三个取值：<code>none</code> 不做处理，输出的结果为向量；<code>mean</code> 将<code>none</code>结果求均值后输出；<code>sum</code> 将<code>none</code> 结果求和后输出。</p>
<ul>
<li><code>torch.nn.CrossEntropyLoss</code>就是上面两个函数的组合<code>nll_loss(log_softmax(input))</code>。</li>
</ul>
<h3 id="二分类交叉熵">二分类交叉熵</h3>
<p>多分类中使用<code>softmax</code>函数将最后的输出映射为每个类别的概率，而在二分类中则通常使用<code>sigmoid</code> 将输出映射为正样本的概率。这是因为二分类中，只有两个类别：{正样本，负样本}，只需要求得正样本的概率<span class="math inline">\(q\)</span>,则<span class="math inline">\(1-q\)</span> 就是负样本的概率。这也是多分类和二分类不同的地方。</p>
<p><span class="math inline">\(\text{sigmoid}\)</span> 函数的表达式如下：<br />
<span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span><br />
sigmoid的输入为<span class="math inline">\(z\)</span> ，其输出为<span class="math inline">\((0,1)\)</span> ，可以表示分类为正样本的概率。</p>
<p>二分类的交叉熵可以看作是交叉熵损失的一个特列，交叉熵为<br />
<span class="math display">\[
\text{Cross_Entropy}(p,q) = -\sum_i^m p(x_i) \log q(x_i)
\]</span><br />
这里只有两个类别<span class="math inline">\(x \in {x_1,x_2}\)</span> ，则有<br />
<span class="math display">\[
\begin{aligned}
\text{Cross_Entropy}(p,q) &amp;= -(p(x_1) \log q(x_1) + p(x_2)  \log q(x_2)) 
\end{aligned}
\]</span><br />
因为只有两个选择，则有<span class="math inline">\(p(x_1) + p(x_2) = 1,q(x_1) + q(x_2) = 1\)</span> 。设，训练样本中<span class="math inline">\(x_1\)</span>的概率为<span class="math inline">\(p\)</span>，则<span class="math inline">\(x_2\)</span>为<span class="math inline">\(1-p\)</span>; 预测的<span class="math inline">\(x_1\)</span>的概率为<span class="math inline">\(q\)</span>，则<span class="math inline">\(x_2\)</span>的预测概率为<span class="math inline">\(1 - q\)</span> 。则上式可改写为<br />
<span class="math display">\[
\text{Cross_Entropy}(p,q) = -(p \log q + (1-p)  \log (1-q))
\]</span><br />
也就是二分类交叉熵的损失函数。</p>
<h3 id="总结">总结</h3>
<p>相对熵可以用来度量两个分布相似性，假设分布<span class="math inline">\(p\)</span>是训练样本的分布，<span class="math inline">\(q\)</span>是预测得到的分布。分类训练的过程实际上就是最小化<span class="math inline">\(D_{KL}(p \parallel q)\)</span>，由于由于交叉熵<br />
<span class="math display">\[H(p,q)= D_{KL}(p \parallel q) + H(p)\]</span><br />
其中,<span class="math inline">\(H(p)\)</span>是训练样本的熵，是一个已知的常量，这样最小化相对熵就等价于最小化交叉熵。</p>
<p>从最大似然估计转化为最小化负对数似然<br />
<span class="math display">\[
\theta_{ML} = -arg \min_\theta E_{x\sim p}\log q(x;\theta)
\]</span><br />
也等价于最小化相对熵。</p>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>