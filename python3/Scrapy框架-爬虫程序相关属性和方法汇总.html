<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修Scrapy框架-爬虫程序相关属性和方法汇总' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>Scrapy框架-爬虫程序相关属性和方法汇总</center></div><div class='banquan'>原文出处:本文由博客园博主小小咸鱼YwY提供。<br/>
原文连接:https://www.cnblogs.com/pythonywy/p/11727332.html</div><br>
    <h2 id="一.爬虫项目类相关属性">一.爬虫项目类相关属性</h2>
<ul>
<li>name:爬虫任务的名称</li>
<li>allowed_domains:允许访问的网站</li>
<li>start_urls: 如果没有指定url，就从该列表中读取url来生成第一个请求</li>
<li>custom_settings:值为一个字典，定义一些配置信息，在运行爬虫程序时，这些配置会覆盖项目级别的配置<br />
所以custom_settings必须被定义成一个类属性，由于settings会在类实例化前被加载</li>
<li>settings:通过self.settings['配置项的名字']可以访问settings.py中的配置，如果自己定义了custom_settings还是以自己的为准</li>
<li>logger:日志名默认为spider的名字</li>
<li>crawler:该属性必须被定义到类方法from_crawler中,crawler可以直接crawler.settings.get('setting文件中的名称')</li>
</ul>
<h2 id="二.爬虫项目类相关方法">二.爬虫项目类相关方法</h2>
<ul>
<li><code>from_crawler(crawler, *args, **kwargs)</code>:这个就是优先于__init__执行函数举例代码可以如下</li>
</ul>
<pre><code><code>#一般配置数据库的属性时候稍微用影响
#简单些下
@classmethod
def from_crawler(cls,crawler):
    HOST = crawler.settings.get(&#39;HOST&#39;) #这里面的属性都是在settings中设置的名称
    PORT = crawler.settings.get(&#39;PORT&#39;)
    USER = crawler.settings.get(&#39;USER&#39;)
    PWD = crawler.settings.get(&#39;PWD&#39;)
    DB = crawler.settings.get(&#39;DB&#39;)
    TABLE = crawler.settings.get(&#39;TABLE&#39;)
    return cls(HOST,PORT,USER,PWD,DB,TABLE)
def __init__(self,HOST,PORT,USER,PWD,DB,TABLE):
    self.HOST = HOST
    self.PORT = PORT
    self.USER = USER
    self.PWD = PWD
    self.DB = DB
    self.TABLE = TABLE
#看一眼就知道了吧</code></pre>
<ul>
<li><code>start_requests(self)</code>:该方法用来发起第一个Requests请求，且必须返回一个可迭代的对象。它在爬虫程序打开时就被Scrapy调用，Scrapy只调用它一次。<br />
默认从start_urls里取出每个url来生成Request(url, dont_filter=True)</li>
</ul>
<p>举例</p>
<p>如果不写<code>start_requests</code>方法:他会把<code>start_urls</code>的两个网址都发送过去</p>
<pre><code><code>import scrapy
class BaiduSpider(scrapy.Spider):
    name = &#39;test&#39;
    allowed_domains = [&#39;http://httpbin.org/get&#39;]
    start_urls = [&#39;http://httpbin.org/get&#39;,&#39;http://httpbin.org/get&#39;]
    
    def parse(self, response):
        print(&#39;接受一次&#39;)
        </code></pre>
<p>如果写<code>start_requests</code>方法:他会把我们指定的Request对象发送出去,发送必须以<code>迭代器</code>的形式输出</p>
<ul>
<li><p><code>parse(self,response)</code>:这是默认的回调函数</p></li>
<li><code>log(self, message, level=logging.DEBUG, **kw):</code> 定义日志级别</li>
<li><p><code>close(self,reason)</code>:关闭爬虫程序执行</p></li>
</ul>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>