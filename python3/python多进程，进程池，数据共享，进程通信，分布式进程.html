<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修python多进程，进程池，数据共享，进程通信，分布式进程' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>python多进程，进程池，数据共享，进程通信，分布式进程</center></div><div class='banquan'>原文出处:本文由博客园博主西瓜你个兔子提供。<br/>
原文连接:https://www.cnblogs.com/FuckSpider/p/11552031.html</div><br>
    <h2 id="一操作系统中相关进程的知识">一、操作系统中相关进程的知识</h2>
<blockquote>
<p>  Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。
  子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。
  Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程。</p>
</blockquote>
<p>  <strong>示例如下</strong></p>
<pre><code><code>import os
pid=os.fork()
if pid==0:
    print(&#39;I am child process %s my parents is %s&#39;%(os.getpid(),os.getppid()))
else:
    print(&#39;I (%s) just created a child process (%s).&#39;%(os.getpid(),pid))</code></pre>
<p>  <strong>输出如下</strong></p>
<pre><code><code>I (64225) just created a child process (64226).
I am child process 64226 my parents is 64225</code></pre>
<h2 id="二跨平台模块multiprocessing">二、跨平台模块multiprocessing</h2>
<p><code>multiprocessing</code>模块提供了一个Process类来代表一个进程对象。
<br />
  <strong>示例1</strong></p>
<pre><code><code>from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print(&#39;Run child process %s (%s)...&#39; % (name, os.getpid()))

if __name__==&#39;__main__&#39;:
    print(&#39;Parent process %s.&#39; % os.getppid())
    p = Process(target=run_proc, args=(&#39;test&#39;,))
    print(&#39;Child process will start.&#39;)
    p.start()
    p.join()
    print(&#39;Child process end.&#39;)
#join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。  </code></pre>
<p>  <strong>示例2</strong></p>
<pre><code><code>from multiprocessing import Process
import time
import os
class P(Process):
    def run(self):
        print(&#39;Run child process %s (%s)...&#39;%(self.name,os.getpid()))  # 默认函数对象有name方法 ，结果为：P-1
        time.sleep(3)
        print(&#39;%s is done&#39; % self.name)
if __name__ == &#39;__main__&#39;:
    print(&#39;Parent process %s.&#39; % os.getppid())
    p=P()
    p.start()
    p.join()</code></pre>
<h2 id="三进程数据隔离">三、进程数据隔离</h2>
<p>多个进程间的数据是隔离的，也就是说多个进程修改全局变量互不影响
<br />
  <strong>验证示例</strong></p>
<pre><code><code>from multiprocessing import Process
import time
x=100
def task():
    global x
    print(&#39;子进程开启，当前x的值为%d&#39;%x)
    time.sleep(3)
    x=10
    print(&#39;子进程结束，当前x的值为%d&#39;%x)

if __name__ == &#39;__main__&#39;:
    print(&#39;当前为父进程，准备开启子进程，x的值为%d&#39; % x)
    p1=Process(target=task)
    p1.start()
    p1.join()
    print(&#39;当前为父进程，准备结束父进程，x的值为%d&#39; % x)</code></pre>
<p>  <strong>输出</strong></p>
<pre><code><code>当前为父进程，准备开启子进程，x的值为100
子进程开启，当前x的值为100
子进程结束，当前x的值为10
当前为父进程，准备结束父进程，x的值为100</code></pre>
<p>==<em>注意：有些情况是需要加锁的情况，如文件读写问题</em>==</p>
<h2 id="四多进程并行执行">四、多进程并行执行</h2>
<p>  <strong>示例如下</strong></p>
<pre><code><code>import time
from multiprocessing import Process

def task(name,n):
    print(&#39;%s is running&#39;%name)
    time.sleep(n)
    print(&#39;%s is done&#39;%name)

if __name__ == &#39;__main__&#39;:
    p1=Process(target=task,args=(&quot;进程1&quot;,1)) #用时1s
    p2=Process(target=task,args=(&quot;进程2&quot;,2)) #用时1s
    p3=Process(target=task,args=(&quot;进程3&quot;,3)) #用时1s
    
    start_time=time.time()
    p1.start()
    p2.start()
    p3.start()
    # 当第一秒在运行p1时，其实p2、p3也已经在运行，当1s后到p2时只需要再运行1s就到p3了，到p3也是一样。
    p1.join()
    p2.join()
    p3.join()
    stop_time=time.time()     
    print(stop_time-start_time) #3.2848567962646484</code></pre>
<h2 id="五进程池">五、进程池</h2>
<h4 id="线性执行-pool.apply">1、线性执行( pool.apply() )</h4>
<pre><code><code>from multiprocessing import Pool  # 导入进程池模块pool
import time,os
def foo(i):
    time.sleep(2)
    print(&quot;in process&quot;, os.getpid())  # 打印进程号
if __name__ == &quot;__main__&quot;:
    pool = Pool(processes=5)   # 设置允许进程池同时放入5个进程
    for i in range(10):
        pool.apply(func=foo, args=(i,))   # 同步执行挂起进程
    print(&#39;end&#39;)
    pool.close() # 关闭进程池，不再接受新进程
    pool.join()  # 进程池中进程执行完毕后再关闭，如果注释掉，那么程序直接关闭。</code></pre>
<h4 id="并发执行-pool.apply_async">2、并发执行（ pool.apply_async() ）</h4>
<pre><code><code>from multiprocessing import Pool  # 导入进程池模块pool
import time,os
def foo(i):
    time.sleep(2)
    print(&quot;in process&quot;, os.getpid())  # 打印进程号
if __name__ == &quot;__main__&quot;:
    pool = Pool(processes=5)   # 设置允许进程池同时放入5个进程，并且将这5个进程交给cpu去运行
    for i in range(10):
        pool.apply_async(func=foo, args=(i,))   # 采用异步方式执行foo函数
    print(&#39;end&#39;)
    pool.close()
    pool.join()  # 进程池中进程执行完毕后再关闭，如果注释掉，那么程序直接关闭。</code></pre>
<h4 id="设置回调">3、设置回调</h4>
<pre><code><code>from multiprocessing import Process,Pool
import time,os
def foo(i):
    time.sleep(2)
    print(&quot;in process&quot;, os.getpid())  # 打印子进程的进程号
def bar(arg):#注意arg参数是必须要有的
    print(&#39;--&gt;exec done:&#39;, arg, os.getpid())   # 打印进程号
 
if __name__ == &quot;__main__&quot;:
    pool = Pool(processes=2)
    print(&quot;主进程&quot;, os.getpid())   # 主进程的进程号
    for i in range(3):
        pool.apply_async(func=foo, args=(i,), callback=bar)   # 执行回调函数callback=Bar
    print(&#39;end&#39;)
    pool.close()
    pool.join()  # 进程池中进程执行完毕后再关闭，如果注释掉，那么程序直接关闭。</code></pre>
<p>  <strong>执行结果</strong></p>
<pre><code><code>主进程 752
end
in process 2348
--&gt;exec done: None 752
in process 8364
--&gt;exec done: None 752
in process 2348
--&gt;exec done: None 752
#回调函数说明fun=Foo干不完就不执行bar函数，等Foo执行完就去执行Bar
#这个回调函数是主进程去调用的，而不是每个子进程去调用的。</code></pre>
<h2 id="六子进程">六、子进程</h2>
<p>  <strong>1、</strong> 很多时候子进程是一个外部进程，如执行一条命令，这和命令行执行效果是一样的
  <strong>示例如下</strong></p>
<pre><code><code>import subprocess
print(&#39;$nslookup https://www.baidu.com&#39;)
r = subprocess.call([&#39;nslookup&#39;,&#39;https://www.baidu.com&#39;])
print(&#39;Exit code&#39;,r)</code></pre>
<p>  <strong>2、</strong> 有时候子进程还需要进行输入，可以通过<code>communicate</code>方法来输入
  <strong>示例如下</strong></p>
<pre><code><code>import subprocess
print(&#39;$ nslookup https://www.baidu.com&#39;)
p = subprocess.Popen([&#39;nslookup&#39;],stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
output,err = p.communicate(b&#39;set q=mx\nbaidu.com\nexit\n&#39;)
print(output.decode(&#39;gbk&#39;))
print(&#39;Exit code:&#39;,p.returncode)</code></pre>
<p>  <strong>输出如下</strong></p>
<pre><code><code>$ nslookup https://www.baidu.com
默认服务器:  bogon
Address:  192.168.111.1

&gt; &gt; 服务器:  bogon
Address:  192.168.111.1

baidu.com   MX preference = 10, mail exchanger = mx.maillb.baidu.com
baidu.com   MX preference = 20, mail exchanger = jpmx.baidu.com
baidu.com   MX preference = 15, mail exchanger = mx.n.shifen.com
baidu.com   MX preference = 20, mail exchanger = mx50.baidu.com
baidu.com   MX preference = 20, mail exchanger = mx1.baidu.com
&gt; 
Exit code: 0</code></pre>
<h2 id="七守护进程">七、守护进程</h2>
<p>守护进程在主进程代码执行完毕时立刻挂掉，然后主进程等待非守护进程执行完毕后回收子进程的资源（避免产生僵尸进程），整体才算结束。<br />
<strong>示例</strong></p>
<pre><code><code>from multiprocessing import Process
import os
import time

def task(x):
    print(&#39;%s is running &#39; %x)
    time.sleep(3)
    print(&#39;%s is done&#39; %x)

if __name__ == &#39;__main__&#39;:
    p1=Process(target=task,args=(&#39;守护进程&#39;,))
    p2=Process(target=task,args=(&#39;子进程&#39;,))
    p2.start()
    p1.daemon=True   # 设置p1为守护进程
    p1.start()
    print(&#39;主进程代码执行完毕&#39;)

&gt;&gt;：主进程代码执行完毕
&gt;&gt;：子进程 is running
&gt;&gt;：子进程 is done</code></pre>
<p>==<em>可以从结果看出，主进程代码执行完，守护进程立即挂掉，主进程在等待子进程执行完毕后退出</em>==</p>
<h2 id="八进程间通信">八、进程间通信</h2>
<p>  如果想要进程间通信可以使用<code>Queue</code>或<code>Pipe</code>来实现
  <strong>使用Queue示例</strong></p>
<pre><code><code>from multiprocessing import Queue,Process
def put_id(q):
     q.put([1,2,3,4])
if __name__ == &#39;__main__&#39;:
     q=Queue()
     p=Process(target=put_id,args=(q,))
     p.start()
     print(q.get())
     p.join()
# 输出
[1,2,3,4]</code></pre>
<p>==<em>注意：在这需要从multiprocessing导入Queue模块</em>==</p>
<p>  <strong>使用Pipe示例</strong></p>
<pre><code><code>from multiprocessing import Process,Pipe
def put_id(conn):
    conn.send([1,2,3])
    conn.send([4,5,6])
    conn.close()
    
if __name__ == &#39;__main__&#39;:
    ## 生成管道。 生成时会产生两个返回对象，这两个对象相当于两端的电话，通过管道线路连接。
    ## 两个对象分别交给两个变量。
    parent_conn,child_conn=Pipe()
    p=Process(target=put_id,args=(child_conn,))#child_conn需要传给对端，用于send数据给parent_conn
    p.start()
    print(parent_conn.recv())  # parent_conn在这断用于接收数据&gt;&gt;&gt;&gt;[1,2,3]
    print(parent_conn.recv())  # parent_conn在这断用于接收数据&gt;&gt;&gt;&gt;[4,5,6]
    p.join()</code></pre>
<p>==<em>注意两端要发送次数和接受次数要对等，不然会卡住直到对等</em>==</p>
<h2 id="九进程间数据共享字典和列表型">九、进程间数据共享（字典和列表型）</h2>
<p>  前面说过，进程间数据是隔离的，如果想要进程间数据共享可以通过<code>Manager</code>来实现
  <strong>示例如下</strong></p>
<pre><code><code>from multiprocessing import Manager,Process
from random import randint
import os
def run(d,l):
    d[randint(1,50)]=randint(51,100)#生成一个可在多个进程之间传递和共享的字典
    l.append(os.getpid())
    print(l)
if __name__ == &#39;__main__&#39;:
    with Manager() as manage: #做一个别名，此时manager就相当于Manager()
        d=manage.dict()#生成一个可在多个进程之间传递和共享的字典
        l=manage.list(range(5))#生成一个可在多个进程之间传递和共享的列表
        p_list=[]
        for i in range(10):#生成10个进程
            p=Process(target=run,args=(d,l))
            p_list.append(p)# 将每个进程放入空列表中
            p.start()
        for i in p_list:
            i.join()
        print(d)#所有进程都执行完毕后打印字典
        print(l)#所有进程都执行完毕后打印列表</code></pre>
<h2 id="十分布式进程">十、分布式进程</h2>
<p>  在做分布式计算时显然进程比线程各合适，一来进程更稳定，二来线程最多只能在同一台机器的多个cpu上运行；
  <code>multiprocessing</code>的<code>managers</code>子模块支持把多进程分布到多个机器上,一个服务进程用作调度者，依靠网络将任务分布到其它多个进程中。
  假设有一个需求，拥有两台机器，一台机器用来做发送任务的服务进程，一台用来做处理任务的服务进程；
  <strong>示例如下</strong></p>
<pre><code><code># task_master.py
from multiprocessing.managers import BaseManager
from queue import Queue
import random
import time

task_queue = Queue()
result_queue = Queue()

class QueueManager(BaseManager):
        pass

def get_task_queue():
    global task_queue
    return task_queue


def get_result_queue():
    global result_queue
    return result_queue


if __name__ == &#39;__main__&#39;:
    # 将两个队列注册到网络上，calltable参数关联Queue对象
    QueueManager.register(&#39;get_task_queue&#39;, callable=get_task_queue)
    QueueManager.register(&#39;get_result_queue&#39;, callable=get_result_queue)

    # 创建一个队列管理器，绑定端口5000，设定密码为abc
    manager = QueueManager(address=(&#39;127.0.0.1&#39;,5000),authkey=b&#39;abc&#39;)
    manager.start()

    # 通过网络获取Queue对象
    task = manager.get_task_queue()
    result = manager.get_result_queue()

    # 放任务进去
    for i in range(10):
        n = random.randint(0,1000)
        print(&#39;Put Task %d&#39;%n)
        task.put(n)

    # 从结果队列获取结果
    print(&#39;Try get results&#39;)
    for i in range(10):
        r = result.get()
        print(&#39;Result: %s&#39; % r)

    manager.shutdown()
    print(&#39;master exit&#39;)</code></pre>
<p>==<em>注意：一定要用注册过的Queue对象，另外在linux/unix/mac等系统上注册可直接使用<code>QueueManager.register('get_result_queue', callable=lambda : result_queue)</code></em>==</p>
<pre><code><code># task_worker.py
from multiprocessing.managers import BaseManager
from queue import Queue
from queue import Empty
import time

class QueueManager(BaseManager):
    pass

if __name__ == &#39;__main__&#39;:
    # 从服务器上获取，所以注册时只需要提供名字，也就是接口名字
    QueueManager.register(&#39;get_task_queue&#39;)
    QueueManager.register(&#39;get_result_queue&#39;)

    # 连接到服务器，也就是task_master.py的机器
    server_addr = &#39;127.0.0.1&#39;
    manager = QueueManager(address=(server_addr,5000),authkey=b&#39;abc&#39;)
    manager.connect()

    # 获取Queue对象
    task = manager.get_task_queue()
    result = manager.get_result_queue()

    # 从队列提取任务，将处理结果插入result队列
    for i in range(10):
        try:
            n = task.get(timeout=1)
            print(&#39;run task %d*%d&#39;%(n,n))
            r = &#39;%d * %d = %d&#39;%(n,n,n*n)
            time.sleep(1)
            result.put(r)
        except Empty:
            print(&#39;task queue is empty&#39;)
    print(&#39;worker exit&#39;)</code></pre>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>