<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修python爬虫（4）——scrapy框架' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>python爬虫（4）——scrapy框架</center></div><div class='banquan'>原文出处:本文由博客园博主swineherd_MCQ提供。<br/>
原文连接:https://www.cnblogs.com/mcq1999/p/11437168.html</div><br>
    <h1 id="安装">安装</h1>
<p>urllib库更适合写爬虫文件，scrapy更适合做爬虫项目。</p>
<p>步骤：</p>
<ol>
<li>先更改pip源，国外的太慢了，参考：https://www.jb51.net/article/159167.htm</li>
<li>升级pip：python -m pip install --upgrade pip</li>
<li>pip install wheel</li>
<li>pip install lxml</li>
<li>pip install Twisted</li>
<li>pip install scrapy</li>
</ol>
<h1 id="常用命令">常用命令</h1>
<p>核心目录<img src="./images/python爬虫（4）——scrapy框架0.png" /></p>
<p><img src="./images/python爬虫（4）——scrapy框架1.png" /></p>
<ol>
<li>新建项目：scrapy startproject mcq</li>
<li>运行独立的爬虫文件（不是项目）：比如</li>
</ol>
<p><img src="./images/python爬虫（4）——scrapy框架2.png" /></p>
<p>然后输入命令scrapy runspider gg.py</p>
<ol>
<li><p>获取设置信息：cd到项目，比如scrapy settings --get BOT_NAME</p></li>
<li><p>交互式爬取：scrapy shell http://www.baidu.com，可以使用python代码</p></li>
<li><p>scrapy版本信息：scrapy version</p></li>
<li><p>爬取并且在浏览器显示：scrapy view http://news.1152.com，将网页下载到本地打开</p></li>
<li><p>测试本地硬件性能：scrapy bench ,每分钟可以爬取多少页面</p></li>
<li><p>依据模板创建爬虫文件：scrapy genspider -l ,有以下模板<img src="./images/python爬虫（4）——scrapy框架3.png" /></p>
<p>选择basic，scrapy genspider -t basic haha baidu.com （注意：这里填可爬取的域名，域名是不以www、edu……开头的）<br />
<img src="./images/python爬虫（4）——scrapy框架4.png" /></p></li>
</ol>
<p><img src="./images/python爬虫（4）——scrapy框架5.png" /></p>
<ol>
<li><p>测试爬虫文件是否合规：scrapy check haha</p></li>
<li><p>运行爬虫项目下的文件：scrapy crawl haha<br />
不显示中间的日志信息：scrapy crawl haha --nolog</p></li>
<li><p>查看当前项目下可用的爬虫文件：scrapy list</p></li>
<li><p>指定某个爬虫文件获取url：<br />
F:\scrapy项目\mcq&gt;scrapy parse --spider=haha http://www.baidu.com</p></li>
</ol>
<h1 id="xpath表达式">XPath表达式</h1>
<p>XPath与正则简单对比：</p>
<ol>
<li>XPath表达式效率会高一点</li>
<li>正则表达式功能强一点</li>
<li>一般来说，优先选择XPath，但是XPath解决不了的问题我们就选正则去解决</li>
</ol>
<p>/：逐层提取</p>
<p>text()提取标签下面的文本</p>
<p>如要提取标题:/html/head/title/text()</p>
<p>//标签名：提取所有名为……的标签</p>
<p>如提取所有的div标签：//div</p>
<p>//标签名[@属性='属性值']：提取属性为……的标签</p>
<p>@属性表示取某个属性值</p>
<h1 id="使用scrapy做当当网商品爬虫">使用scrapy做当当网商品爬虫</h1>
<p>新建爬虫项目：F:\scrapy项目&gt;scrapy startproject dangdang</p>
<p>F:\scrapy项目&gt;cd dangdang</p>
<p>F:\scrapy项目\dangdang&gt;scrapy genspider -t basic dd dangdang.com</p>
<p><img src="./images/python爬虫（4）——scrapy框架6.png" /></p>
<p>修改items.py：</p>
<pre><code><code># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class DangdangItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title=scrapy.Field() #商品标题
    link=scrapy.Field() #商品链接
    comment=scrapy.Field() #商品评论
    </code></pre>
<p>我们翻一下页，分析两个链接：</p>
<p><a href="http://category.dangdang.com/pg2-cid4008154.html" class="uri">http://category.dangdang.com/pg2-cid4008154.html</a></p>
<p>http://category.dangdang.com/pg3-cid4008154.html</p>
<p>可以找到初始链接：http://category.dangdang.com/pg1-cid4008154.html</p>
<p>分析页面源码，可以从name=&quot;itemlist-title&quot; 下手，因为这个正好有48个结果，即一页商品的数量。</p>
<p>ctrl+f 条评论，可以发现正好有48条记录。</p>
<p>dd.py：</p>
<pre><code><code># -*- coding: utf-8 -*-
import scrapy
from dangdang.items import DangdangItem
from scrapy.http import Request
class DdSpider(scrapy.Spider):
    name = &#39;dd&#39;
    allowed_domains = [&#39;dangdang.com&#39;]
    start_urls = [&#39;http://category.dangdang.com/pg1-cid4008154.html&#39;]

    def parse(self, response):
        item=DangdangItem()
        item[&quot;title&quot;]=response.xpath(&quot;//a[@name=&#39;itemlist-title&#39;]/@title&quot;).extract()
        item[&quot;link&quot;]=response.xpath(&quot;//a[@name=&#39;itemlist-title&#39;]/@href&quot;).extract()
        item[&quot;comment&quot;]=response.xpath(&quot;//a[@name=&#39;itemlist-review&#39;]/text()&quot;).extract()
        # print(item[&quot;title&quot;])
        yield item
        for i in range(2,11): #爬取2~10页
            url=&#39;http://category.dangdang.com/pg&#39;+str(i)+&#39;-cid4008154.html&#39;
            yield Request(url, callback=self.parse)
</code></pre>
<p>对于dd里的Request：</p>
<p>url: 就是需要请求，并进行下一步处理的url<br />
callback: 指定该请求返回的Response，由那个函数来处理。</p>
<p>先把settings.py的robots改为False:<img src="./images/python爬虫（4）——scrapy框架7.png" /></p>
<p>settings.py:</p>
<pre><code><code># -*- coding: utf-8 -*-

# Scrapy settings for dangdang project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = &#39;dangdang&#39;

SPIDER_MODULES = [&#39;dangdang.spiders&#39;]
NEWSPIDER_MODULE = &#39;dangdang.spiders&#39;


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = &#39;dangdang (+http://www.yourdomain.com)&#39;

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
#   &#39;Accept-Language&#39;: &#39;en&#39;,
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    &#39;dangdang.middlewares.DangdangSpiderMiddleware&#39;: 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    &#39;dangdang.middlewares.DangdangDownloaderMiddleware&#39;: 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html

ITEM_PIPELINES = {
   &#39;dangdang.pipelines.DangdangPipeline&#39;: 300,
}
# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = &#39;httpcache&#39;
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;</code></pre>
<p>运行：F:\scrapy项目\dangdang&gt;scrapy crawl dd --nolog</p>
<p>去settings.py将pipeline开启：<img src="./images/python爬虫（4）——scrapy框架8.png" /></p>
<p>pipelines.py:</p>
<pre><code><code># -*- coding: utf-8 -*-
import pymysql
# Define your item pipelines here
#
# Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


class DangdangPipeline(object):
    def process_item(self, item, spider):
        conn=pymysql.connect(host=&#39;127.0.0.1&#39;,user=&quot;root&quot;,passwd=&quot;123456&quot;,db=&quot;dangdang&quot;)
        cursor = conn.cursor()
        for i in range(len(item[&quot;title&quot;])):
            title=item[&quot;title&quot;][i]
            link=item[&quot;link&quot;][i]
            comment=item[&quot;comment&quot;][i]
            # print(title+&quot;:&quot;+link+&quot;:&quot;+comment)
            sql=&quot;insert into goods(title,link,comment) values(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(title,link,comment)
            # print(sql)
            try:
                cursor.execute(sql)
                conn.commit()
            except Exception as e:
                print(e)
        conn.close()
        return item
</code></pre>
<p>登录mysql，创建一个数据库：mysql&gt; create database dangdang;</p>
<p>mysql&gt; use dangdang</p>
<p>mysql&gt; create table goods(id int(32) auto_increment primary key,title varchar(100),link varchar(100) unique,comment varchar(100));</p>
<p>最后运行 scrapy crawl dd --nolog</p>
<p><img src="./images/python爬虫（4）——scrapy框架9.png" /></p>
<p>每页48条，48*10=480，爬取成功！</p>
<p><strong>完整项目源代码参考我的github</strong></p>
<h1 id="scrapy模拟登陆实战">scrapy模拟登陆实战</h1>
<p>以这个网站为例http://edu.iqianyue.com/，我们不爬取内容，只模拟登陆，所以不需要写item.py</p>
<p>点击登陆，用fiddler查看真正的登陆网址：http://edu.iqianyue.com/index_user_login</p>
<p><img src="./images/python爬虫（4）——scrapy框架10.png" /></p>
<p>修改login.py：</p>
<pre><code><code># -*- coding: utf-8 -*-
import scrapy
from scrapy.http import FormRequest, Request


class LoginSpider(scrapy.Spider):
    name = &#39;login&#39;
    allowed_domains = [&#39;iqianyue.com&#39;]
    start_urls = [&#39;http://iqianyue.com/&#39;]
    header={&quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0&quot;}
    #编写start_request()方法，第一次会默认调取该方法中的请求
    def start_requests(self):
        #首先爬一次登录页，然后进入回调函数parse()
        return [Request(&quot;http://edu.iqianyue.com/index_user_login&quot;,meta={&quot;cookiejar&quot;:1},callback=self.parse)]
    def parse(self, response):
        #设置要传递的post信息，此时没有验证码字段
        data={
            &quot;number&quot;:&quot;swineherd&quot;,
            &quot;passwd&quot;:&quot;123&quot;,
        }
        print(&quot;登录中……&quot;)
        #通过ForRequest.from_response()进行登录
        return FormRequest.from_response(response,
                                          #设置cookie信息
                                          meta={&quot;cookiejar&quot;:response.meta[&quot;cookiejar&quot;]},
                                          #设置headers信息模拟成浏览器
                                          headers=self.header,
                                          #设置post表单中的数据
                                          formdata=data,
                                          #设置回调函数
                                          callback=self.next,
                                          )
    def next(self,response):
        data=response.body
        fp=open(&quot;a.html&quot;,&quot;wb&quot;)
        fp.write(data)
        fp.close()
        print(response.xpath(&quot;/html/head/title/text()&quot;).extract())
        #登录后访问
        yield Request(&quot;http://edu.iqianyue.com/index_user_index&quot;,callback=self.next2,meta={&quot;cookiejar&quot;:1})
    def next2(self,response):
        data=response.body
        fp=open(&quot;b.html&quot;,&quot;wb&quot;)
        fp.write(data)
        fp.close()
        print(response.xpath(&quot;/html/head/title/text()&quot;).extract())</code></pre>
<p><img src="./images/python爬虫（4）——scrapy框架11.png" /></p>
<h1 id="scrapy新闻爬虫实战">scrapy新闻爬虫实战</h1>
<p>目标：爬取百度新闻首页所有新闻</p>
<p>F:&gt;cd scrapy项目</p>
<p>F:\scrapy项目&gt;scrapy startproject baidunews</p>
<p>F:\scrapy项目&gt;cd baidunews</p>
<p>F:\scrapy项目\baidunews&gt;scrapy genspider -t basic n1 baidu.com</p>
<p>抓包分析</p>
<p>找到json文件：<br />
<img src="./images/python爬虫（4）——scrapy框架12.png" /></p>
<p>idle查看一下<img src="./images/python爬虫（4）——scrapy框架13.png" /></p>
<p>首页ctrl+f:<img src="./images/python爬虫（4）——scrapy框架14.png" /></p>
<p>在首页往下拖触发所有新闻，在fiddler中找到存储url、title等等的js文件（并不是每一个js文件都有用）</p>
<p>发现不止js文件有新闻信息，还有别的，要细心在fiddler找！</p>
<p><a href="http://news.baidu.com/widget?id=LocalNews&amp;ajax=json&amp;t=1566824493194" class="uri">http://news.baidu.com/widget?id=LocalNews&amp;ajax=json&amp;t=1566824493194</a></p>
<p><a href="http://news.baidu.com/widget?id=civilnews&amp;t=1566824634139" class="uri">http://news.baidu.com/widget?id=civilnews&amp;t=1566824634139</a></p>
<p><a href="http://news.baidu.com/widget?id=InternationalNews&amp;t=1566824931323" class="uri">http://news.baidu.com/widget?id=InternationalNews&amp;t=1566824931323</a></p>
<p><a href="http://news.baidu.com/widget?id=EnterNews&amp;t=1566824931341" class="uri">http://news.baidu.com/widget?id=EnterNews&amp;t=1566824931341</a></p>
<p><a href="http://news.baidu.com/widget?id=SportNews&amp;t=1566824931358" class="uri">http://news.baidu.com/widget?id=SportNews&amp;t=1566824931358</a></p>
<p><a href="http://news.baidu.com/widget?id=FinanceNews&amp;t=1566824931376" class="uri">http://news.baidu.com/widget?id=FinanceNews&amp;t=1566824931376</a></p>
<p><a href="http://news.baidu.com/widget?id=TechNews&amp;t=1566824931407" class="uri">http://news.baidu.com/widget?id=TechNews&amp;t=1566824931407</a></p>
<p><a href="http://news.baidu.com/widget?id=MilitaryNews&amp;t=1566824931439" class="uri">http://news.baidu.com/widget?id=MilitaryNews&amp;t=1566824931439</a></p>
<p><a href="http://news.baidu.com/widget?id=InternetNews&amp;t=1566824931456" class="uri">http://news.baidu.com/widget?id=InternetNews&amp;t=1566824931456</a></p>
<p><a href="http://news.baidu.com/widget?id=DiscoveryNews&amp;t=1566824931473" class="uri">http://news.baidu.com/widget?id=DiscoveryNews&amp;t=1566824931473</a></p>
<p><a href="http://news.baidu.com/widget?id=LadyNews&amp;t=1566824931490" class="uri">http://news.baidu.com/widget?id=LadyNews&amp;t=1566824931490</a></p>
<p><a href="http://news.baidu.com/widget?id=HealthNews&amp;t=1566824931506" class="uri">http://news.baidu.com/widget?id=HealthNews&amp;t=1566824931506</a></p>
<p><a href="http://news.baidu.com/widget?id=PicWall&amp;t=1566824931522" class="uri">http://news.baidu.com/widget?id=PicWall&amp;t=1566824931522</a></p>
<p>我们可以发现真正影响新闻信息的是widget?后面的id值</p>
<p>写个脚本把id提取出来：<img src="./images/python爬虫（4）——scrapy框架15.png" /></p>
<p>两种不同的链接的源代码的url也不同：</p>
<p><img src="./images/python爬虫（4）——scrapy框架16.png" /></p>
<p><img src="./images/python爬虫（4）——scrapy框架17.png" /></p>
<p>items.py:</p>
<pre><code><code># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class BaidunewsItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title=scrapy.Field()
    link=scrapy.Field()
    content=scrapy.Field()</code></pre>
<p>n1.py:</p>
<pre><code><code># -*- coding: utf-8 -*-
import scrapy
from baidunews.items import BaidunewsItem #从核心目录
from scrapy.http import Request
import re
import time
class N1Spider(scrapy.Spider):
    name = &#39;n1&#39;
    allowed_domains = [&#39;baidu.com&#39;]
    start_urls = [&quot;http://news.baidu.com/widget?id=LocalNews&amp;ajax=json&quot;]
    allid=[&#39;LocalNews&#39;, &#39;civilnews&#39;, &#39;InternationalNews&#39;, &#39;EnterNews&#39;, &#39;SportNews&#39;, &#39;FinanceNews&#39;, &#39;TechNews&#39;, &#39;MilitaryNews&#39;, &#39;InternetNews&#39;, &#39;DiscoveryNews&#39;, &#39;LadyNews&#39;, &#39;HealthNews&#39;, &#39;PicWall&#39;]
    allurl=[]
    for k in range(len(allid)):
        thisurl=&quot;http://news.baidu.com/widget?id=&quot;+allid[k]+&quot;&amp;ajax=json&quot;
        allurl.append(thisurl)

    def parse(self, response):
        while True: #每隔5分钟爬一次
            for m in range(len(self.allurl)):
                yield Request(self.allurl[m], callback=self.next)
                time.sleep(300) #单位为秒
    cnt=0
    def next(self,response):
        print(&quot;第&quot; + str(self.cnt) + &quot;个栏目&quot;)
        self.cnt+=1
        data=response.body.decode(&quot;utf-8&quot;,&quot;ignore&quot;)
        pat1=&#39;&quot;m_url&quot;:&quot;(.*?)&quot;&#39;
        pat2=&#39;&quot;url&quot;:&quot;(.*?)&quot;&#39;
        url1=re.compile(pat1,re.S).findall(data)
        url2=re.compile(pat2,re.S).findall(data)
        if(len(url1)!=0):
            url=url1
        else :
            url=url2
        for i in range(len(url)):
            thisurl=re.sub(&quot;\\\/&quot;,&quot;/&quot;,url[i])
            print(thisurl)
            yield Request(thisurl,callback=self.next2)
    def next2(self,response):
        item=BaidunewsItem()
        item[&quot;link&quot;]=response.url
        item[&quot;title&quot;]=response.xpath(&quot;/html/head/title/text()&quot;)
        item[&quot;content&quot;]=response.body
        print(item)
        yield item</code></pre>
<p>将settings的pipeline开启：<img src="./images/python爬虫（4）——scrapy框架18.png" /></p>
<p>将robots改为False，scrapy crawl n1 --nolog即可运行</p>
<h1 id="scrapy豆瓣网登录爬虫">scrapy豆瓣网登录爬虫</h1>
<p>要在settings里加上：</p>
<p>USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5'</p>
<p>关于scrapy.http.FormRequest和scrapy.http.FormRequest.from_response的用法区别参考这篇博客：https://blog.csdn.net/qq_33472765/article/details/80958820</p>
<p>d1.py:</p>
<pre><code><code># -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request, FormRequest


class D1Spider(scrapy.Spider):
    name = &#39;d1&#39;
    allowed_domains = [&#39;douban.com&#39;]
    # start_urls = [&#39;http://douban.com/&#39;]
    headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0&quot;}

    def start_requests(self):
        # 首先爬一次登录页，然后进入回调函数parse()
        print(&quot;开始：&quot;)
        return [Request(&quot;https://accounts.douban.com/passport/login&quot;,meta={&quot;cookiejar&quot;:1},callback=self.login)]

    def login(self, response):
        #判断验证码
        captcha=response.xpath(&quot;//&quot;)
        data = {
            &quot;ck&quot;: &quot;&quot;,
            &quot;name&quot;: &quot;***&quot;,
            &quot;password&quot;: &quot;***&quot;,
            &quot;remember&quot;: &quot;false&quot;,
            &quot;ticket&quot;: &quot;&quot;
        }
        print(&quot;登陆中……&quot;)
        return FormRequest(url=&quot;https://accounts.douban.com/j/mobile/login/basic&quot;,
                                         # 设置cookie信息
                                         meta={&quot;cookiejar&quot;: response.meta[&quot;cookiejar&quot;]},
                                         # 设置headers信息模拟成浏览器
                                         headers=self.headers,
                                         # 设置post表单中的数据
                                         formdata=data,
                                         # 设置回调函数
                                         callback=self.next,
                                         )
    def next(self,response):
        #跳转到个人中心
        yield Request(&quot;https://www.douban.com/people/202921494/&quot;,meta={&quot;cookiejar&quot;:1},callback=self.next2)
    def next2(self, response):
        title = response.xpath(&quot;/html/head/title/text()&quot;).extract()
        print(title)</code></pre>
<p>现在的豆瓣是滑块验证码，对于现在的我这个菜鸡还不会处理。</p>
<h1 id="在urllib中使用xpath表达式">在urllib中使用XPath表达式</h1>
<p>先安装lxml模块：pip install lxml，然后将网页数据通过lxml下的etree转化为treedata的形式。</p>
<pre><code><code>import urllib.request
from lxml import etree
data=urllib.request.urlopen(&quot;http://www.baidu.com&quot;).read().decode(&quot;utf-8&quot;,&quot;ignore&quot;)
treedata=etree.HTML(data)
title=treedata.xpath(&quot;//title/text()&quot;)
print(title)</code></pre>

</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>